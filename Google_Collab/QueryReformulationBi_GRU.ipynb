{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QueryReformulationAtten.ipynb","provenance":[{"file_id":"15aHNisUNc_9CkxOoJuJz7y6FpUvjXsPa","timestamp":1606175284948}],"machine_shape":"hm","authorship_tag":"ABX9TyNvNjnMKGSlDMCofPNsNbrP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05Ofx8ojFkje","executionInfo":{"status":"ok","timestamp":1607443273179,"user_tz":300,"elapsed":11820,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"6f43f8be-9a92-4b21-eb91-8109b7fbeaa0"},"source":["%%shell\n","wget https://obj.umiacs.umd.edu/elgohary/CANARD_Release.zip\n","unzip CANARD_Release.zip\n","rm CANARD_Release.zip\n","rm -r __MACOSX\n","\n","pip install spacy\n","python -m spacy download en\n","\n","mkdir data\n","mkdir data/seq2seq"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-12-08 16:01:01--  https://obj.umiacs.umd.edu/elgohary/CANARD_Release.zip\n","Resolving obj.umiacs.umd.edu (obj.umiacs.umd.edu)... 128.8.122.11\n","Connecting to obj.umiacs.umd.edu (obj.umiacs.umd.edu)|128.8.122.11|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3258983 (3.1M) [application/zip]\n","Saving to: ‘CANARD_Release.zip’\n","\n","CANARD_Release.zip  100%[===================>]   3.11M  4.97MB/s    in 0.6s    \n","\n","2020-12-08 16:01:02 (4.97 MB/s) - ‘CANARD_Release.zip’ saved [3258983/3258983]\n","\n","Archive:  CANARD_Release.zip\n","   creating: CANARD_Release/\n","  inflating: __MACOSX/._CANARD_Release  \n","  inflating: CANARD_Release/multiple_refs.json  \n","  inflating: __MACOSX/CANARD_Release/._multiple_refs.json  \n","  inflating: CANARD_Release/test.json  \n","  inflating: __MACOSX/CANARD_Release/._test.json  \n","  inflating: CANARD_Release/dev.json  \n","  inflating: __MACOSX/CANARD_Release/._dev.json  \n","  inflating: CANARD_Release/train.json  \n","  inflating: __MACOSX/CANARD_Release/._train.json  \n","  inflating: CANARD_Release/readme.txt  \n","  inflating: __MACOSX/CANARD_Release/._readme.txt  \n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.4)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n","Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.6/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"T351i-3R7IKI","executionInfo":{"status":"ok","timestamp":1607443556127,"user_tz":300,"elapsed":754,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["import json\n","import argparse\n","from os.path import join\n","from spacy.lang.en import English\n","import csv\n","\n","def preprocess(dataset_files, output_dir, split):\n","  nlp = English()\n","\n","  with open(join(output_dir,'{}.tsv').format(split), 'w') as outfile:\n","    tsv_writer = csv.writer(outfile, delimiter='\\t')\n","\n","    for file in dataset_files:\n","      with open(file) as inh:\n","        samples = json.load(inh)\n","    \n","      for sample in samples:\n","          src = ' ||| '.join(sample['History']+[sample['Question']])\n","          tgt = sample['Rewrite']\n","          src = ' '.join([tok.text for tok in nlp(src)])\n","          tgt = ' '.join([tok.text for tok in nlp(tgt)])\n","          tsv_writer.writerow([src, tgt])"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VGuvkni7YVP","executionInfo":{"status":"ok","timestamp":1607443604176,"user_tz":300,"elapsed":42306,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"564fe185-1461-4e9f-f31b-885bd3faecec"},"source":["preprocess(['CANARD_Release/train.json', 'CANARD_Release/dev.json'], 'data/seq2seq', 'train-dev')\n","preprocess(['CANARD_Release/train.json'], 'data/seq2seq', 'train')\n","preprocess(['CANARD_Release/dev.json'], 'data/seq2seq', 'dev')\n","preprocess(['CANARD_Release/test.json'], 'data/seq2seq', 'test')\n","\n","!wc -l /content/data/seq2seq/train-dev.tsv\n","!wc -l /content/data/seq2seq/train.tsv\n","!wc -l /content/data/seq2seq/dev.tsv\n","!wc -l /content/data/seq2seq/test.tsv"],"execution_count":3,"outputs":[{"output_type":"stream","text":["34956 /content/data/seq2seq/train-dev.tsv\n","31526 /content/data/seq2seq/train.tsv\n","3430 /content/data/seq2seq/dev.tsv\n","5571 /content/data/seq2seq/test.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b59Ou_jp8r5F","executionInfo":{"status":"ok","timestamp":1607443938390,"user_tz":300,"elapsed":27570,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"d54bd6ef-0cb7-41e8-ca41-c0610fff4933"},"source":["import torch\n","import torchtext\n","from torchtext import data\n","import spacy\n","\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","init_token = '<sos>'\n","eos_token = '<eos>'\n","\n","TEXT = data.Field(sequential=True, tokenize=\"spacy\", init_token=init_token, eos_token=eos_token, lower=True, batch_first=True, include_lengths=True)\n","\n","# train, val, test = data.TabularDataset.splits(\n","#         path='data/seq2seq', train='train.tsv',\n","#         validation='dev.tsv', test='test.tsv', format='tsv',\n","#         fields=[('Source', TEXT), ('Target', TEXT)])\n","\n","train, val = data.TabularDataset.splits(\n","        path='data/seq2seq', train='train-dev.tsv',\n","        validation='test.tsv', format='tsv',\n","        fields=[('Source', TEXT), ('Target', TEXT)])\n","\n","\n","print(vars(train[0]))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["{'Source': ['johnny', 'unitas', '|||', '1964', 'mvp', 'season', '|||', 'what', 'team', 'did', 'unitas', 'play', 'for'], 'Target': ['what', 'team', 'did', 'johnny', 'unitas', 'play', 'for', '?']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBt4bMhvT5E1","executionInfo":{"status":"ok","timestamp":1607445302603,"user_tz":300,"elapsed":1322974,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"5cb76fd1-a80a-44d8-f826-fe8ff3715618"},"source":["TEXT.build_vocab(train, val, min_freq = 2, vectors=\"glove.840B.300d\")\n","\n","print()\n","print(len(TEXT.vocab.stoi))\n","\n","print(train[0].__dict__.keys())\n","print(train[0].Source)"],"execution_count":5,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.840B.300d.zip: 2.18GB [16:58, 2.14MB/s]                            \n","100%|█████████▉| 2195023/2196017 [03:50<00:00, 9887.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","33069\n","dict_keys(['Source', 'Target'])\n","['johnny', 'unitas', '|||', '1964', 'mvp', 'season', '|||', 'what', 'team', 'did', 'unitas', 'play', 'for']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dcxlgwku3YUR","executionInfo":{"status":"ok","timestamp":1607445577417,"user_tz":300,"elapsed":774,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["BATCH_SIZE = 16\n","train_iterator, valid_iterator = data.BucketIterator.splits(\n","    (train, val),\n","    batch_size=BATCH_SIZE,\n","    sort_within_batch = True,\n","    sort_key = lambda x : len(x.Source),\n","    shuffle=True,\n","    device=device)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VvA0rpr9PAi","executionInfo":{"status":"ok","timestamp":1607445594602,"user_tz":300,"elapsed":8594,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"d3ee4fda-5d60-439a-d6f9-2acfaaa0c0cc"},"source":["batch = next(iter(train_iterator))\n","print(batch.Target[0].shape)\n","\n","print('Training data has {} batches'.format(len(train_iterator)))\n","print('Validation data has {} batches'.format(len(valid_iterator)))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["torch.Size([16, 21])\n","Training data has 2185 batches\n","Validation data has 349 batches\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VFKL2UOR1hcz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607445615911,"user_tz":300,"elapsed":18447,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"db95567e-110e-46ea-98ad-18e2037f7daa"},"source":["import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.nn.functional as F\n","import random\n","import time\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2eYzyUFzpdFw","executionInfo":{"status":"ok","timestamp":1607446618546,"user_tz":300,"elapsed":684,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["class EncoderBiRNN(nn.Module):\n","  def __init__(self, pretrained_embed, padding_idx, enc_hid_dim, dec_hid_dim, fix = True, dropout=0.0):\n","    super(EncoderBiRNN, self).__init__()\n","    self.vocab_size, self.embedding_dim = pretrained_embed.size()\n","    self.enc_hid_dim = enc_hid_dim\n","    self.dec_hid_dim = dec_hid_dim\n","    self.dropout = dropout\n","\n","    self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n","    self.embedding.padding_idx = padding_idx\n","    if fix:\n","      self.embedding.weight.requires_grad = False\n","\n","    self.gru = nn.GRU(self.embedding_dim, enc_hid_dim, batch_first=True, bidirectional=True)\n","    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","    self.dropout = nn.Dropout(dropout)\n","  \n","\n","  def forward(self, encoder_input, src_len, hidden):\n","    #src_len = [batch size]\n","\n","    embedded = self.embedding(encoder_input)\n","\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.cpu(), batch_first=True)\n","\n","    packed_outputs, hidden = self.gru(packed_embedded, hidden)\n","\n","    outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n","    outputs = outputs.permute(1,0,2)\n","\n","    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","\n","    #outputs = [batch size, src sent len, enc hid dim * 2]\n","    #hidden = [batch size, dec hid dim]\n","\n","    return outputs, hidden\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"28eoQjn-SRpF","executionInfo":{"status":"ok","timestamp":1607446620630,"user_tz":300,"elapsed":974,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["class Attention(nn.Module):\n","  def __init__(self, enc_hid_dim, dec_hid_dim, attn_dim):\n","    super(Attention, self).__init__()\n","\n","    self.enc_hid_dim = enc_hid_dim\n","    self.dec_hid_dim = dec_hid_dim\n","    self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n","    # self.attn = nn.Linear(self.attn_in, attn_dim)\n","            \n","    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n","    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n","\n","  def forward(self, decoder_hidden, encoder_outputs, mask):\n","    batch_size = encoder_outputs.shape[0]\n","\n","    #decoder_hidden = [batch size, src sent len, dec hid dim]\n","    #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n","    # print('attn decoder_hidden', decoder_hidden.shape)\n","\n","    #repeat decoder hidden state src_len times\n","    repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, encoder_outputs.shape[1], 1)\n","    # print('atten repeated_decoder', repeated_decoder_hidden.shape)\n","\n","    # Step 1: to enable feeding through \"self.attn\", concatenate \n","    # `repeated_decoder_hidden` and `encoder_outputs`:\n","    # torch.cat((hidden, encoder_outputs), dim = 2) has shape \n","    # [batch_size, seq_len, enc_hid_dim * 2 + dec_hid_dim]\n","\n","    # Step 2: feed through self.attn to end up with:\n","    # [batch_size, seq_len, attn_dim]\n","\n","    # Step 3: feed through tanh\n","    energy = torch.tanh(self.attn(torch.cat((\n","            repeated_decoder_hidden, \n","            encoder_outputs), \n","            dim = 2)))\n","    #energy = [batch size, src len, dec hid dim]\n","\n","    #attention = torch.sum(energy, dim=2)\n","    attention = self.v(energy).squeeze(2)\n","\n","    # print('atten shape', attention.shape)\n","    #attention= [batch size, src len]\n","    attention = attention.masked_fill(mask == 0, -1e10)\n","\n","    return F.softmax(attention, dim=1)\n","        \n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_oH2WEaPm5u","executionInfo":{"status":"ok","timestamp":1607446622349,"user_tz":300,"elapsed":505,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["class Decoder(nn.Module):\n","  def __init__(self, pretrained_embed, padding_idx, enc_hid_dim, dec_hid_dim, attention, fix=True, dropout=0.0):\n","    super(Decoder, self).__init__()\n","    self.enc_hid_dim = enc_hid_dim\n","    self.dec_hid_dim = dec_hid_dim\n","    self.attention = attention\n","\n","    self.vocab_size, self.embedding_dim = pretrained_embed.size()\n","\n","    self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n","    self.embedding.padding_idx = padding_idx\n","    if fix:\n","      self.embedding.weight.requires_grad = False\n","\n","    self.gru = nn.GRU((enc_hid_dim * 2) + self.embedding_dim, dec_hid_dim, batch_first=True)\n","\n","    self.out = nn.Linear(self.attention.attn_in + self.embedding_dim, self.vocab_size)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, decoder_hidden, encoder_outputs, mask):\n","    #x = [batch size] Note: \"one character at a time\"\n","    #hidden = [batch size, dec hid dim]\n","    # print('x', x.shape)\n","    # print('decoder_hidden', decoder_hidden.shape)\n","\n","    x = x.unsqueeze(1)\n","    # print('x', x.shape)\n","    embedded = self.embedding(x)\n","    # print('embedded', embedded.shape)\n","\n","    a = self.attention(decoder_hidden, encoder_outputs, mask)\n","    a = a.unsqueeze(1)\n","    weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n","\n","    rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n","    #rnn_input = [batch size, 1, (enc hid dim * 2) + emb dim]\n","    # print('rnn_input', rnn_input.shape)\n","    \n","    output, decoder_hidden = self.gru(rnn_input, decoder_hidden.unsqueeze(0))\n","    #output = [batch size, sent len, dec hid dim * n directions]\n","    #decoder_hidden = [n layers * n directions, batch size, dec hid dim]\n","\n","    # print('decoder output', output.shape)\n","    # print('decoder hidden', decoder_hidden.shape)\n","\n","    embedded = embedded.squeeze(1)\n","    output = output.squeeze(1)\n","    weighted_encoder_rep = weighted_encoder_rep.squeeze(1)\n","    output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim = 1))\n","    # print('decoder output', output.shape)\n","    #output = [bsz, output dim]\n","    return output, decoder_hidden.squeeze(0), a.squeeze(1)\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVY7n9EmCtNK","executionInfo":{"status":"ok","timestamp":1607446624586,"user_tz":300,"elapsed":758,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, src_pad_idx, device):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.enc_hid_dim = encoder.enc_hid_dim\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.device = device\n","\n","    def create_mask(self, src):\n","      mask = (src != self.src_pad_idx)\n","      return mask\n","\n","    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n","      batch_size = src.shape[0]\n","      max_len = trg.shape[1]\n","      trg_vocab_size = self.decoder.vocab_size\n","\n","      outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","      initHidden = torch.zeros((2, batch_size, self.enc_hid_dim)).to(self.device)\n","      # print('s2s initHidden', initHidden.shape)\n","      enc_output, hidden = self.encoder(src, src_len, initHidden)\n","      # print('hidden', hidden.shape)\n","\n","      dec_input = trg[:,0]\n","\n","      mask = self.create_mask(src)\n","\n","      for t in range(1, max_len):\n","        pred, hidden, _ = self.decoder(dec_input, hidden, enc_output, mask)\n","        # print('pred', pred.shape)\n","        # print('hidden', hidden.shape)\n","        outputs[t] = pred\n","\n","        teacher_force = random.random() < teacher_forcing_ratio\n","        top1 = pred.max(1)[1]\n","        dec_input = trg[:, t] if teacher_force else top1\n","  \n","      return outputs \n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZLX0IyIW6fJ","executionInfo":{"status":"ok","timestamp":1607446626795,"user_tz":300,"elapsed":802,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"sqg4Sd8bFK3_","executionInfo":{"status":"ok","timestamp":1607446630096,"user_tz":300,"elapsed":516,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["def train_epoch(epoch, model, data_iterator, optimizer, criterion):\n","  model.train()\n","  running_loss = 0.0\n","\n","  for i, batch in enumerate(data_iterator):\n","    src, src_len = batch.Source\n","    trg, _ = batch.Target\n","\n","    optimizer.zero_grad()\n","\n","    output = model(src, src_len, trg)\n","\n","    trg = trg.permute(1,0)\n","    trg = trg[1:].reshape(-1)\n","\n","    output = output[1:].view(-1, output.shape[-1])\n","\n","    loss = criterion(output, trg)\n","    \n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","\n","    optimizer.step()\n","\n","    running_loss += loss.item()\n","\n","    if i % 100 == 0:\n","      print('Batch ', i)\n","      print('train_loss: {:.8f}'.format(loss.item()))\n","\n","  epoch_loss = running_loss / len(data_iterator)\n","  print(\"Epoch: {} | train_loss:{:.8f}\".format(epoch, epoch_loss))\n","\n","  return epoch_loss\n","\n","\n","def eval_epoch(epoch, model, data_iterator, scheduler, criterion):\n","  model.eval()\n","  running_loss = 0.0\n","\n","  with torch.no_grad():\n","    for i, batch in enumerate(data_iterator):\n","      src, src_len = batch.Source\n","      trg, _ = batch.Target\n","\n","      output = model(src, src_len, trg, 0) #turn off teacher forcing\n","      output = output[1:].reshape(-1, output.shape[-1])\n","\n","      trg = trg.permute(1,0)\n","      trg = trg[1:].reshape(-1)\n","\n","      loss = criterion(output, trg)\n","      running_loss += loss.item()\n","\n","      if i % 100 == 0:\n","        print('Batch ', i)\n","        print('val_loss: {:.8f}'.format(loss.item()))\n","\n","  epoch_loss = running_loss / len(data_iterator)\n","  print(\"Epoch: {} | val_loss:{:.8f}\".format(epoch, epoch_loss))\n","  scheduler.step(epoch_loss)\n","  print('scheduler adjust learning rate to ', optimizer.param_groups[0]['lr'])\n","\n","  return epoch_loss\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"brgpfVtCOx2b","executionInfo":{"status":"error","timestamp":1607477996006,"user_tz":300,"elapsed":3923773,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"2c6677a8-f7a6-42be-e3f5-6f5410f285a2"},"source":["enc_hid_dim = 512\n","dec_hid_dim = 512\n","attn_dim = 64\n","PAD_idx = TEXT.vocab.stoi['<pad>']\n","attn = Attention(enc_hid_dim, dec_hid_dim, attn_dim)\n","enc = EncoderBiRNN(pretrained_embed=TEXT.vocab.vectors, padding_idx=TEXT.vocab.stoi[TEXT.pad_token], enc_hid_dim=enc_hid_dim, dec_hid_dim=dec_hid_dim)\n","dec = Decoder(pretrained_embed=TEXT.vocab.vectors, padding_idx=TEXT.vocab.stoi[TEXT.pad_token], enc_hid_dim=enc_hid_dim, dec_hid_dim=dec_hid_dim, attention=attn)\n","model = Seq2Seq(enc, dec, PAD_idx, device).to(device)\n","\n","\n","learning_rate = 1e-3\n","#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience = 50, min_lr=1e-9)\n","criterion = nn.CrossEntropyLoss(ignore_index = TEXT.vocab.stoi['<pad>'])\n","\n","epoches = 2000\n","best_loss = 5\n","plot_losses = []\n","\n","print('total parameters.',  sum(p.numel() for p in model.parameters() if p.requires_grad))\n","print(\"training batches: \", len(train_iterator))\n","print(\"val batches: \", len(valid_iterator))\n","\n","\n","for epoch in range(epoches):\n","  start_time = time.time()\n","  train_epoch_loss = train_epoch(epoch,model, train_iterator, optimizer, criterion)\n","  eval_epoch_loss = eval_epoch(epoch, model, valid_iterator, scheduler, criterion)\n","  end_time = time.time()\n","\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","  plot_losses.append(eval_epoch_loss)\n","\n","  if (eval_epoch_loss < best_loss) or epoch % 10 == 0:\n","    if eval_epoch_loss < best_loss:\n","      best_loss = eval_epoch_loss\n","    torch.save(model.state_dict(), '/content/drive/MyDrive/gru/model_{:.8f}_{}.pt'.format(eval_epoch_loss, epoch))\n","\n","  print(f'*************Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s ***********')\n","\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["total parameters. 67383785\n","training batches:  2185\n","val batches:  349\n","Batch  0\n","train_loss: 10.40270042\n","Batch  100\n","train_loss: 5.31844807\n","Batch  200\n","train_loss: 5.75977659\n","Batch  300\n","train_loss: 5.20387268\n","Batch  400\n","train_loss: 5.25387621\n","Batch  500\n","train_loss: 4.57339430\n","Batch  600\n","train_loss: 4.37727928\n","Batch  700\n","train_loss: 4.60602236\n","Batch  800\n","train_loss: 4.56505442\n","Batch  900\n","train_loss: 3.21207333\n","Batch  1000\n","train_loss: 3.07398844\n","Batch  1100\n","train_loss: 4.25958443\n","Batch  1200\n","train_loss: 4.19718838\n","Batch  1300\n","train_loss: 4.22239733\n","Batch  1400\n","train_loss: 2.62138152\n","Batch  1500\n","train_loss: 2.99777508\n","Batch  1600\n","train_loss: 3.41293740\n","Batch  1700\n","train_loss: 3.65928960\n","Batch  1800\n","train_loss: 3.04116321\n","Batch  1900\n","train_loss: 3.44119787\n","Batch  2000\n","train_loss: 2.27388930\n","Batch  2100\n","train_loss: 2.86063862\n","Epoch: 0 | train_loss:3.81118659\n","Batch  0\n","val_loss: 3.51596832\n","Batch  100\n","val_loss: 4.15853119\n","Batch  200\n","val_loss: 4.43392754\n","Batch  300\n","val_loss: 6.12960243\n","Epoch: 0 | val_loss:4.87146487\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 01 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 1.78779972\n","Batch  100\n","train_loss: 2.60969186\n","Batch  200\n","train_loss: 0.89357257\n","Batch  300\n","train_loss: 0.67682582\n","Batch  400\n","train_loss: 2.58089471\n","Batch  500\n","train_loss: 2.10370111\n","Batch  600\n","train_loss: 2.43805003\n","Batch  700\n","train_loss: 2.66046548\n","Batch  800\n","train_loss: 2.70792031\n","Batch  900\n","train_loss: 2.53850937\n","Batch  1000\n","train_loss: 0.92151338\n","Batch  1100\n","train_loss: 2.00184107\n","Batch  1200\n","train_loss: 2.30063915\n","Batch  1300\n","train_loss: 2.34777451\n","Batch  1400\n","train_loss: 1.41201532\n","Batch  1500\n","train_loss: 3.19919086\n","Batch  1600\n","train_loss: 2.03277111\n","Batch  1700\n","train_loss: 3.05029178\n","Batch  1800\n","train_loss: 2.69912195\n","Batch  1900\n","train_loss: 1.14848757\n","Batch  2000\n","train_loss: 2.68904734\n","Batch  2100\n","train_loss: 1.99440503\n","Epoch: 1 | train_loss:2.09459846\n","Batch  0\n","val_loss: 3.29217410\n","Batch  100\n","val_loss: 4.28231668\n","Batch  200\n","val_loss: 4.39329815\n","Batch  300\n","val_loss: 6.37238503\n","Epoch: 1 | val_loss:4.94174545\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 02 | Time: 6m 27s ***********\n","Batch  0\n","train_loss: 1.44485033\n","Batch  100\n","train_loss: 1.62570596\n","Batch  200\n","train_loss: 1.99138606\n","Batch  300\n","train_loss: 1.67474043\n","Batch  400\n","train_loss: 2.29968500\n","Batch  500\n","train_loss: 2.00770903\n","Batch  600\n","train_loss: 0.74776721\n","Batch  700\n","train_loss: 2.22582841\n","Batch  800\n","train_loss: 2.19886899\n","Batch  900\n","train_loss: 1.68216908\n","Batch  1000\n","train_loss: 0.76017988\n","Batch  1100\n","train_loss: 0.74207765\n","Batch  1200\n","train_loss: 1.68806446\n","Batch  1300\n","train_loss: 1.44835806\n","Batch  1400\n","train_loss: 1.33242404\n","Batch  1500\n","train_loss: 1.63013887\n","Batch  1600\n","train_loss: 1.59719563\n","Batch  1700\n","train_loss: 2.14958334\n","Batch  1800\n","train_loss: 1.65404904\n","Batch  1900\n","train_loss: 1.61225212\n","Batch  2000\n","train_loss: 1.39638162\n","Batch  2100\n","train_loss: 2.03086686\n","Epoch: 2 | train_loss:1.59440633\n","Batch  0\n","val_loss: 3.47829914\n","Batch  100\n","val_loss: 4.77176762\n","Batch  200\n","val_loss: 4.53190327\n","Batch  300\n","val_loss: 6.45127869\n","Epoch: 2 | val_loss:5.20729560\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 03 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.72570521\n","Batch  100\n","train_loss: 0.94188243\n","Batch  200\n","train_loss: 1.66498721\n","Batch  300\n","train_loss: 0.58391577\n","Batch  400\n","train_loss: 1.22234344\n","Batch  500\n","train_loss: 0.52851528\n","Batch  600\n","train_loss: 0.61110169\n","Batch  700\n","train_loss: 1.51115263\n","Batch  800\n","train_loss: 0.58151531\n","Batch  900\n","train_loss: 0.92639297\n","Batch  1000\n","train_loss: 1.19928217\n","Batch  1100\n","train_loss: 2.11025715\n","Batch  1200\n","train_loss: 1.30607450\n","Batch  1300\n","train_loss: 1.12381923\n","Batch  1400\n","train_loss: 1.67918372\n","Batch  1500\n","train_loss: 0.82727593\n","Batch  1600\n","train_loss: 1.32848501\n","Batch  1700\n","train_loss: 1.03388250\n","Batch  1800\n","train_loss: 0.42378366\n","Batch  1900\n","train_loss: 0.97677851\n","Batch  2000\n","train_loss: 2.41724920\n","Batch  2100\n","train_loss: 1.81561506\n","Epoch: 3 | train_loss:1.35782445\n","Batch  0\n","val_loss: 3.65466714\n","Batch  100\n","val_loss: 4.27199984\n","Batch  200\n","val_loss: 4.71645689\n","Batch  300\n","val_loss: 6.45128536\n","Epoch: 3 | val_loss:5.35247215\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 04 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 1.20789027\n","Batch  100\n","train_loss: 1.20876253\n","Batch  200\n","train_loss: 1.70003259\n","Batch  300\n","train_loss: 1.35073721\n","Batch  400\n","train_loss: 1.36929905\n","Batch  500\n","train_loss: 1.20105100\n","Batch  600\n","train_loss: 1.28696525\n","Batch  700\n","train_loss: 1.13301432\n","Batch  800\n","train_loss: 1.33443785\n","Batch  900\n","train_loss: 1.14511597\n","Batch  1000\n","train_loss: 0.37938946\n","Batch  1100\n","train_loss: 0.87998945\n","Batch  1200\n","train_loss: 0.26436239\n","Batch  1300\n","train_loss: 0.70862490\n","Batch  1400\n","train_loss: 1.98501825\n","Batch  1500\n","train_loss: 0.86401385\n","Batch  1600\n","train_loss: 0.86958021\n","Batch  1700\n","train_loss: 1.57962263\n","Batch  1800\n","train_loss: 1.39171267\n","Batch  1900\n","train_loss: 0.43438751\n","Batch  2000\n","train_loss: 0.98679781\n","Batch  2100\n","train_loss: 1.06474507\n","Epoch: 4 | train_loss:1.19268611\n","Batch  0\n","val_loss: 3.68333673\n","Batch  100\n","val_loss: 4.85130930\n","Batch  200\n","val_loss: 5.00022459\n","Batch  300\n","val_loss: 6.99063396\n","Epoch: 4 | val_loss:5.55838856\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 05 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.75805968\n","Batch  100\n","train_loss: 1.05628169\n","Batch  200\n","train_loss: 1.15566766\n","Batch  300\n","train_loss: 1.04083192\n","Batch  400\n","train_loss: 1.11503470\n","Batch  500\n","train_loss: 0.86215448\n","Batch  600\n","train_loss: 0.50003910\n","Batch  700\n","train_loss: 1.28571367\n","Batch  800\n","train_loss: 1.25062418\n","Batch  900\n","train_loss: 0.66824567\n","Batch  1000\n","train_loss: 0.40974894\n","Batch  1100\n","train_loss: 1.33848488\n","Batch  1200\n","train_loss: 1.72492266\n","Batch  1300\n","train_loss: 0.56824821\n","Batch  1400\n","train_loss: 0.98205823\n","Batch  1500\n","train_loss: 0.30051079\n","Batch  1600\n","train_loss: 0.86349410\n","Batch  1700\n","train_loss: 1.12171853\n","Batch  1800\n","train_loss: 0.82027626\n","Batch  1900\n","train_loss: 1.30199873\n","Batch  2000\n","train_loss: 0.16656005\n","Batch  2100\n","train_loss: 1.09634626\n","Epoch: 5 | train_loss:1.05592770\n","Batch  0\n","val_loss: 4.10596037\n","Batch  100\n","val_loss: 5.02760458\n","Batch  200\n","val_loss: 4.44640160\n","Batch  300\n","val_loss: 6.73433876\n","Epoch: 5 | val_loss:5.74239545\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 06 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 1.24522245\n","Batch  100\n","train_loss: 1.09030712\n","Batch  200\n","train_loss: 0.61170101\n","Batch  300\n","train_loss: 1.75662494\n","Batch  400\n","train_loss: 0.44579840\n","Batch  500\n","train_loss: 0.94482458\n","Batch  600\n","train_loss: 1.08201098\n","Batch  700\n","train_loss: 1.44682479\n","Batch  800\n","train_loss: 0.83638108\n","Batch  900\n","train_loss: 1.31487620\n","Batch  1000\n","train_loss: 0.90667534\n","Batch  1100\n","train_loss: 0.89259869\n","Batch  1200\n","train_loss: 0.70834947\n","Batch  1300\n","train_loss: 1.06090987\n","Batch  1400\n","train_loss: 1.39878452\n","Batch  1500\n","train_loss: 0.50496835\n","Batch  1600\n","train_loss: 0.43849072\n","Batch  1700\n","train_loss: 1.62103963\n","Batch  1800\n","train_loss: 2.04863548\n","Batch  1900\n","train_loss: 0.32029206\n","Batch  2000\n","train_loss: 0.43349501\n","Batch  2100\n","train_loss: 0.82850981\n","Epoch: 6 | train_loss:0.97563176\n","Batch  0\n","val_loss: 3.88798213\n","Batch  100\n","val_loss: 5.36330843\n","Batch  200\n","val_loss: 4.92433691\n","Batch  300\n","val_loss: 7.46997881\n","Epoch: 6 | val_loss:5.88418233\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 07 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.70425993\n","Batch  100\n","train_loss: 1.17480671\n","Batch  200\n","train_loss: 1.37751734\n","Batch  300\n","train_loss: 1.39512932\n","Batch  400\n","train_loss: 0.69138753\n","Batch  500\n","train_loss: 1.55122018\n","Batch  600\n","train_loss: 0.39843234\n","Batch  700\n","train_loss: 0.76872426\n","Batch  800\n","train_loss: 0.50947160\n","Batch  900\n","train_loss: 1.39266217\n","Batch  1000\n","train_loss: 1.31985855\n","Batch  1100\n","train_loss: 0.76219416\n","Batch  1200\n","train_loss: 0.90660703\n","Batch  1300\n","train_loss: 0.26972693\n","Batch  1400\n","train_loss: 0.82027316\n","Batch  1500\n","train_loss: 0.61130387\n","Batch  1600\n","train_loss: 1.08215201\n","Batch  1700\n","train_loss: 1.12821233\n","Batch  1800\n","train_loss: 1.38691056\n","Batch  1900\n","train_loss: 1.82877147\n","Batch  2000\n","train_loss: 0.93478656\n","Batch  2100\n","train_loss: 1.72960210\n","Epoch: 7 | train_loss:0.89084182\n","Batch  0\n","val_loss: 4.15663624\n","Batch  100\n","val_loss: 5.01442480\n","Batch  200\n","val_loss: 5.61291790\n","Batch  300\n","val_loss: 7.00929689\n","Epoch: 7 | val_loss:5.96673208\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 08 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.73183310\n","Batch  100\n","train_loss: 1.18941998\n","Batch  200\n","train_loss: 0.58599544\n","Batch  300\n","train_loss: 1.38308918\n","Batch  400\n","train_loss: 1.47544253\n","Batch  500\n","train_loss: 1.27040315\n","Batch  600\n","train_loss: 1.00721920\n","Batch  700\n","train_loss: 0.43040448\n","Batch  800\n","train_loss: 1.03130198\n","Batch  900\n","train_loss: 0.74421746\n","Batch  1000\n","train_loss: 0.50665540\n","Batch  1100\n","train_loss: 0.92971218\n","Batch  1200\n","train_loss: 1.00266564\n","Batch  1300\n","train_loss: 0.21240446\n","Batch  1400\n","train_loss: 0.97559953\n","Batch  1500\n","train_loss: 1.70552015\n","Batch  1600\n","train_loss: 0.21053736\n","Batch  1700\n","train_loss: 1.61200404\n","Batch  1800\n","train_loss: 0.96866727\n","Batch  1900\n","train_loss: 1.25350666\n","Batch  2000\n","train_loss: 0.95166242\n","Batch  2100\n","train_loss: 1.18591821\n","Epoch: 8 | train_loss:0.82732873\n","Batch  0\n","val_loss: 3.93117929\n","Batch  100\n","val_loss: 5.27186966\n","Batch  200\n","val_loss: 5.48831463\n","Batch  300\n","val_loss: 7.38180780\n","Epoch: 8 | val_loss:6.06247744\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 09 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.49388364\n","Batch  100\n","train_loss: 0.68408203\n","Batch  200\n","train_loss: 1.08475900\n","Batch  300\n","train_loss: 0.74815339\n","Batch  400\n","train_loss: 0.79342067\n","Batch  500\n","train_loss: 0.61616731\n","Batch  600\n","train_loss: 0.98589903\n","Batch  700\n","train_loss: 1.11059880\n","Batch  800\n","train_loss: 0.80779684\n","Batch  900\n","train_loss: 0.31861269\n","Batch  1000\n","train_loss: 0.86589700\n","Batch  1100\n","train_loss: 0.86769950\n","Batch  1200\n","train_loss: 0.91835499\n","Batch  1300\n","train_loss: 0.36815003\n","Batch  1400\n","train_loss: 0.77247572\n","Batch  1500\n","train_loss: 1.37412095\n","Batch  1600\n","train_loss: 1.04652548\n","Batch  1700\n","train_loss: 0.97935688\n","Batch  1800\n","train_loss: 1.44484365\n","Batch  1900\n","train_loss: 0.84818453\n","Batch  2000\n","train_loss: 1.01300097\n","Batch  2100\n","train_loss: 0.31916156\n","Epoch: 9 | train_loss:0.78085248\n","Batch  0\n","val_loss: 4.25959206\n","Batch  100\n","val_loss: 5.25513697\n","Batch  200\n","val_loss: 5.88294697\n","Batch  300\n","val_loss: 7.28324795\n","Epoch: 9 | val_loss:6.17389743\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 10 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.77375787\n","Batch  100\n","train_loss: 1.32260716\n","Batch  200\n","train_loss: 1.05671906\n","Batch  300\n","train_loss: 0.88703048\n","Batch  400\n","train_loss: 0.05814722\n","Batch  500\n","train_loss: 0.06182005\n","Batch  600\n","train_loss: 0.27506074\n","Batch  700\n","train_loss: 0.82324243\n","Batch  800\n","train_loss: 0.55773497\n","Batch  900\n","train_loss: 0.87590945\n","Batch  1000\n","train_loss: 0.59855980\n","Batch  1100\n","train_loss: 0.48294830\n","Batch  1200\n","train_loss: 0.76578009\n","Batch  1300\n","train_loss: 0.20139417\n","Batch  1400\n","train_loss: 0.59083349\n","Batch  1500\n","train_loss: 0.66358483\n","Batch  1600\n","train_loss: 0.62863779\n","Batch  1700\n","train_loss: 0.62586665\n","Batch  1800\n","train_loss: 0.59890610\n","Batch  1900\n","train_loss: 0.95857590\n","Batch  2000\n","train_loss: 0.90570158\n","Batch  2100\n","train_loss: 0.46714458\n","Epoch: 10 | train_loss:0.73917576\n","Batch  0\n","val_loss: 3.98558426\n","Batch  100\n","val_loss: 4.72267103\n","Batch  200\n","val_loss: 5.74894762\n","Batch  300\n","val_loss: 6.70450926\n","Epoch: 10 | val_loss:6.17392294\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 11 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.80349249\n","Batch  100\n","train_loss: 0.75370097\n","Batch  200\n","train_loss: 1.21443856\n","Batch  300\n","train_loss: 0.25343505\n","Batch  400\n","train_loss: 1.09050536\n","Batch  500\n","train_loss: 0.59747159\n","Batch  600\n","train_loss: 0.40519166\n","Batch  700\n","train_loss: 0.77134413\n","Batch  800\n","train_loss: 0.91135192\n","Batch  900\n","train_loss: 1.00076389\n","Batch  1000\n","train_loss: 0.76473421\n","Batch  1100\n","train_loss: 0.39698172\n","Batch  1200\n","train_loss: 0.93862432\n","Batch  1300\n","train_loss: 0.87652731\n","Batch  1400\n","train_loss: 0.95078439\n","Batch  1500\n","train_loss: 0.88616312\n","Batch  1600\n","train_loss: 0.40067330\n","Batch  1700\n","train_loss: 0.72958255\n","Batch  1800\n","train_loss: 1.00170588\n","Batch  1900\n","train_loss: 1.20700729\n","Batch  2000\n","train_loss: 0.20464976\n","Batch  2100\n","train_loss: 0.52992004\n","Epoch: 11 | train_loss:0.72453800\n","Batch  0\n","val_loss: 4.17828274\n","Batch  100\n","val_loss: 5.51375198\n","Batch  200\n","val_loss: 5.07911539\n","Batch  300\n","val_loss: 7.82089520\n","Epoch: 11 | val_loss:6.40608554\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 12 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.56885344\n","Batch  100\n","train_loss: 0.86421615\n","Batch  200\n","train_loss: 0.83970845\n","Batch  300\n","train_loss: 0.81404632\n","Batch  400\n","train_loss: 0.63826418\n","Batch  500\n","train_loss: 0.09563487\n","Batch  600\n","train_loss: 0.78425074\n","Batch  700\n","train_loss: 0.24495630\n","Batch  800\n","train_loss: 0.72268820\n","Batch  900\n","train_loss: 0.41920325\n","Batch  1000\n","train_loss: 0.76834428\n","Batch  1100\n","train_loss: 0.92918354\n","Batch  1200\n","train_loss: 0.84147322\n","Batch  1300\n","train_loss: 0.87341154\n","Batch  1400\n","train_loss: 0.57797122\n","Batch  1500\n","train_loss: 0.74184757\n","Batch  1600\n","train_loss: 0.55908114\n","Batch  1700\n","train_loss: 0.37146509\n","Batch  1800\n","train_loss: 0.38462609\n","Batch  1900\n","train_loss: 0.55672032\n","Batch  2000\n","train_loss: 0.41707829\n","Batch  2100\n","train_loss: 1.14567614\n","Epoch: 12 | train_loss:0.69715035\n","Batch  0\n","val_loss: 4.07783604\n","Batch  100\n","val_loss: 5.49380112\n","Batch  200\n","val_loss: 5.96577501\n","Batch  300\n","val_loss: 7.36446667\n","Epoch: 12 | val_loss:6.36518996\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 13 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.71065336\n","Batch  100\n","train_loss: 0.55973393\n","Batch  200\n","train_loss: 0.75993168\n","Batch  300\n","train_loss: 0.35391906\n","Batch  400\n","train_loss: 0.13395071\n","Batch  500\n","train_loss: 0.88450336\n","Batch  600\n","train_loss: 1.03904724\n","Batch  700\n","train_loss: 0.52763015\n","Batch  800\n","train_loss: 1.31132793\n","Batch  900\n","train_loss: 0.59183651\n","Batch  1000\n","train_loss: 0.76947504\n","Batch  1100\n","train_loss: 1.05570781\n","Batch  1200\n","train_loss: 1.08173919\n","Batch  1300\n","train_loss: 0.07402835\n","Batch  1400\n","train_loss: 0.93091607\n","Batch  1500\n","train_loss: 0.87573999\n","Batch  1600\n","train_loss: 1.50800169\n","Batch  1700\n","train_loss: 1.07353878\n","Batch  1800\n","train_loss: 1.03116798\n","Batch  1900\n","train_loss: 0.13899173\n","Batch  2000\n","train_loss: 0.44200188\n","Batch  2100\n","train_loss: 1.58311546\n","Epoch: 13 | train_loss:0.67888400\n","Batch  0\n","val_loss: 4.15287304\n","Batch  100\n","val_loss: 5.70796537\n","Batch  200\n","val_loss: 5.45176077\n","Batch  300\n","val_loss: 7.66084480\n","Epoch: 13 | val_loss:6.45060002\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 14 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.62010372\n","Batch  100\n","train_loss: 0.31013224\n","Batch  200\n","train_loss: 0.37341639\n","Batch  300\n","train_loss: 1.44283867\n","Batch  400\n","train_loss: 0.27551106\n","Batch  500\n","train_loss: 0.55842781\n","Batch  600\n","train_loss: 0.92975438\n","Batch  700\n","train_loss: 0.87373161\n","Batch  800\n","train_loss: 0.55639678\n","Batch  900\n","train_loss: 0.73889631\n","Batch  1000\n","train_loss: 1.21946549\n","Batch  1100\n","train_loss: 0.08826128\n","Batch  1200\n","train_loss: 0.50147039\n","Batch  1300\n","train_loss: 1.02284634\n","Batch  1400\n","train_loss: 0.60855007\n","Batch  1500\n","train_loss: 0.62601376\n","Batch  1600\n","train_loss: 0.37368941\n","Batch  1700\n","train_loss: 0.56131631\n","Batch  1800\n","train_loss: 0.35314348\n","Batch  1900\n","train_loss: 0.47783256\n","Batch  2000\n","train_loss: 0.49398255\n","Batch  2100\n","train_loss: 0.97719121\n","Epoch: 14 | train_loss:0.65553101\n","Batch  0\n","val_loss: 4.41325235\n","Batch  100\n","val_loss: 5.44707108\n","Batch  200\n","val_loss: 6.15595484\n","Batch  300\n","val_loss: 7.63730860\n","Epoch: 14 | val_loss:6.57162315\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 15 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.28500551\n","Batch  100\n","train_loss: 0.28350812\n","Batch  200\n","train_loss: 1.25919366\n","Batch  300\n","train_loss: 0.64167577\n","Batch  400\n","train_loss: 0.47361755\n","Batch  500\n","train_loss: 0.21423666\n","Batch  600\n","train_loss: 0.93153286\n","Batch  700\n","train_loss: 0.30746144\n","Batch  800\n","train_loss: 0.42053434\n","Batch  900\n","train_loss: 0.33606175\n","Batch  1000\n","train_loss: 0.79808754\n","Batch  1100\n","train_loss: 0.60313785\n","Batch  1200\n","train_loss: 0.96433330\n","Batch  1300\n","train_loss: 0.10387836\n","Batch  1400\n","train_loss: 0.33944148\n","Batch  1500\n","train_loss: 0.54713547\n","Batch  1600\n","train_loss: 1.20599055\n","Batch  1700\n","train_loss: 1.00384974\n","Batch  1800\n","train_loss: 0.68307537\n","Batch  1900\n","train_loss: 0.25967434\n","Batch  2000\n","train_loss: 0.56451523\n","Batch  2100\n","train_loss: 0.25278610\n","Epoch: 15 | train_loss:0.64445072\n","Batch  0\n","val_loss: 4.40442467\n","Batch  100\n","val_loss: 5.57887745\n","Batch  200\n","val_loss: 5.74987125\n","Batch  300\n","val_loss: 7.72762823\n","Epoch: 15 | val_loss:6.51027473\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 16 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.82325482\n","Batch  100\n","train_loss: 1.36451209\n","Batch  200\n","train_loss: 0.41210899\n","Batch  300\n","train_loss: 0.08336426\n","Batch  400\n","train_loss: 0.70432770\n","Batch  500\n","train_loss: 0.64436889\n","Batch  600\n","train_loss: 0.25943875\n","Batch  700\n","train_loss: 0.11731355\n","Batch  800\n","train_loss: 0.34955138\n","Batch  900\n","train_loss: 0.63104701\n","Batch  1000\n","train_loss: 0.70839804\n","Batch  1100\n","train_loss: 0.55949581\n","Batch  1200\n","train_loss: 1.01137817\n","Batch  1300\n","train_loss: 0.42418945\n","Batch  1400\n","train_loss: 0.33064124\n","Batch  1500\n","train_loss: 0.91249990\n","Batch  1600\n","train_loss: 0.85026008\n","Batch  1700\n","train_loss: 0.45552042\n","Batch  1800\n","train_loss: 1.11699295\n","Batch  1900\n","train_loss: 0.73009682\n","Batch  2000\n","train_loss: 0.30672914\n","Batch  2100\n","train_loss: 0.48599261\n","Epoch: 16 | train_loss:0.64843417\n","Batch  0\n","val_loss: 3.89244556\n","Batch  100\n","val_loss: 6.26932764\n","Batch  200\n","val_loss: 5.53473425\n","Batch  300\n","val_loss: 7.63886547\n","Epoch: 16 | val_loss:6.58452550\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 17 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.31552598\n","Batch  100\n","train_loss: 0.72207624\n","Batch  200\n","train_loss: 0.57231224\n","Batch  300\n","train_loss: 0.97070938\n","Batch  400\n","train_loss: 0.47125354\n","Batch  500\n","train_loss: 1.15222180\n","Batch  600\n","train_loss: 0.40042341\n","Batch  700\n","train_loss: 1.62471306\n","Batch  800\n","train_loss: 1.16231179\n","Batch  900\n","train_loss: 0.82736528\n","Batch  1000\n","train_loss: 0.41424519\n","Batch  1100\n","train_loss: 1.18456018\n","Batch  1200\n","train_loss: 0.54922873\n","Batch  1300\n","train_loss: 0.71909904\n","Batch  1400\n","train_loss: 0.53065026\n","Batch  1500\n","train_loss: 1.09457731\n","Batch  1600\n","train_loss: 0.81087619\n","Batch  1700\n","train_loss: 0.31830886\n","Batch  1800\n","train_loss: 1.06131017\n","Batch  1900\n","train_loss: 0.72274339\n","Batch  2000\n","train_loss: 0.56944335\n","Batch  2100\n","train_loss: 0.97689348\n","Epoch: 17 | train_loss:0.63447829\n","Batch  0\n","val_loss: 4.25839043\n","Batch  100\n","val_loss: 5.95544624\n","Batch  200\n","val_loss: 6.43635130\n","Batch  300\n","val_loss: 7.98779106\n","Epoch: 17 | val_loss:6.64459453\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 18 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.82901156\n","Batch  100\n","train_loss: 0.69887596\n","Batch  200\n","train_loss: 0.83039165\n","Batch  300\n","train_loss: 0.57153600\n","Batch  400\n","train_loss: 0.45871374\n","Batch  500\n","train_loss: 0.69658238\n","Batch  600\n","train_loss: 1.36724293\n","Batch  700\n","train_loss: 0.68568176\n","Batch  800\n","train_loss: 0.27548406\n","Batch  900\n","train_loss: 0.81109911\n","Batch  1000\n","train_loss: 1.33098352\n","Batch  1100\n","train_loss: 0.25550616\n","Batch  1200\n","train_loss: 0.73981363\n","Batch  1300\n","train_loss: 0.75163651\n","Batch  1400\n","train_loss: 1.10003412\n","Batch  1500\n","train_loss: 1.79303920\n","Batch  1600\n","train_loss: 0.88699651\n","Batch  1700\n","train_loss: 0.08434590\n","Batch  1800\n","train_loss: 0.51557636\n","Batch  1900\n","train_loss: 0.35206521\n","Batch  2000\n","train_loss: 0.43083397\n","Batch  2100\n","train_loss: 0.67220265\n","Epoch: 18 | train_loss:0.63186140\n","Batch  0\n","val_loss: 4.19187260\n","Batch  100\n","val_loss: 5.57193422\n","Batch  200\n","val_loss: 5.93079376\n","Batch  300\n","val_loss: 7.89192295\n","Epoch: 18 | val_loss:6.65202290\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 19 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.29390970\n","Batch  100\n","train_loss: 0.45718598\n","Batch  200\n","train_loss: 1.64912581\n","Batch  300\n","train_loss: 0.41675559\n","Batch  400\n","train_loss: 0.36576635\n","Batch  500\n","train_loss: 0.53703415\n","Batch  600\n","train_loss: 0.64050525\n","Batch  700\n","train_loss: 0.54881018\n","Batch  800\n","train_loss: 0.40060076\n","Batch  900\n","train_loss: 0.29486400\n","Batch  1000\n","train_loss: 1.44456112\n","Batch  1100\n","train_loss: 0.27602968\n","Batch  1200\n","train_loss: 0.21887483\n","Batch  1300\n","train_loss: 0.81451011\n","Batch  1400\n","train_loss: 0.38746622\n","Batch  1500\n","train_loss: 0.17464593\n","Batch  1600\n","train_loss: 0.67298514\n","Batch  1700\n","train_loss: 0.16487108\n","Batch  1800\n","train_loss: 0.61271983\n","Batch  1900\n","train_loss: 0.41750062\n","Batch  2000\n","train_loss: 0.25469318\n","Batch  2100\n","train_loss: 0.02714081\n","Epoch: 19 | train_loss:0.62459624\n","Batch  0\n","val_loss: 4.39308596\n","Batch  100\n","val_loss: 6.03078461\n","Batch  200\n","val_loss: 6.13689709\n","Batch  300\n","val_loss: 7.99581957\n","Epoch: 19 | val_loss:6.69695275\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 20 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 1.07430983\n","Batch  100\n","train_loss: 0.11654954\n","Batch  200\n","train_loss: 0.88371295\n","Batch  300\n","train_loss: 0.65155071\n","Batch  400\n","train_loss: 0.41778728\n","Batch  500\n","train_loss: 0.54411149\n","Batch  600\n","train_loss: 0.25842866\n","Batch  700\n","train_loss: 0.67739153\n","Batch  800\n","train_loss: 0.64852798\n","Batch  900\n","train_loss: 0.71775746\n","Batch  1000\n","train_loss: 1.27580595\n","Batch  1100\n","train_loss: 0.45377147\n","Batch  1200\n","train_loss: 0.46595287\n","Batch  1300\n","train_loss: 0.04799153\n","Batch  1400\n","train_loss: 0.36952722\n","Batch  1500\n","train_loss: 0.68444699\n","Batch  1600\n","train_loss: 0.38091165\n","Batch  1700\n","train_loss: 0.39708230\n","Batch  1800\n","train_loss: 0.77151889\n","Batch  1900\n","train_loss: 0.40590253\n","Batch  2000\n","train_loss: 0.65895963\n","Batch  2100\n","train_loss: 0.56767863\n","Epoch: 20 | train_loss:0.62359170\n","Batch  0\n","val_loss: 4.10872841\n","Batch  100\n","val_loss: 4.92539454\n","Batch  200\n","val_loss: 6.28583050\n","Batch  300\n","val_loss: 7.96434498\n","Epoch: 20 | val_loss:6.66974635\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 21 | Time: 6m 9s ***********\n","Batch  0\n","train_loss: 0.90778977\n","Batch  100\n","train_loss: 0.39073047\n","Batch  200\n","train_loss: 0.06874776\n","Batch  300\n","train_loss: 0.82495481\n","Batch  400\n","train_loss: 0.69404149\n","Batch  500\n","train_loss: 0.18944921\n","Batch  600\n","train_loss: 0.74706596\n","Batch  700\n","train_loss: 0.49907449\n","Batch  800\n","train_loss: 0.04733779\n","Batch  900\n","train_loss: 0.89795786\n","Batch  1000\n","train_loss: 0.37446567\n","Batch  1100\n","train_loss: 0.40887508\n","Batch  1200\n","train_loss: 0.84454221\n","Batch  1300\n","train_loss: 0.10292493\n","Batch  1400\n","train_loss: 0.68697482\n","Batch  1500\n","train_loss: 0.68626058\n","Batch  1600\n","train_loss: 0.37558982\n","Batch  1700\n","train_loss: 0.45051819\n","Batch  1800\n","train_loss: 0.84465951\n","Batch  1900\n","train_loss: 0.60293823\n","Batch  2000\n","train_loss: 1.49228191\n","Batch  2100\n","train_loss: 0.59136963\n","Epoch: 21 | train_loss:0.63218174\n","Batch  0\n","val_loss: 4.28724432\n","Batch  100\n","val_loss: 5.89024448\n","Batch  200\n","val_loss: 6.12198687\n","Batch  300\n","val_loss: 8.33652878\n","Epoch: 21 | val_loss:6.77919398\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 22 | Time: 6m 7s ***********\n","Batch  0\n","train_loss: 0.40800649\n","Batch  100\n","train_loss: 0.62470216\n","Batch  200\n","train_loss: 0.10646879\n","Batch  300\n","train_loss: 0.53327435\n","Batch  400\n","train_loss: 0.77843267\n","Batch  500\n","train_loss: 0.61198878\n","Batch  600\n","train_loss: 1.13934243\n","Batch  700\n","train_loss: 0.25795117\n","Batch  800\n","train_loss: 0.76208431\n","Batch  900\n","train_loss: 0.47412145\n","Batch  1000\n","train_loss: 0.19660051\n","Batch  1100\n","train_loss: 0.59471202\n","Batch  1200\n","train_loss: 1.01845574\n","Batch  1300\n","train_loss: 0.36244279\n","Batch  1400\n","train_loss: 0.62462741\n","Batch  1500\n","train_loss: 0.69847584\n","Batch  1600\n","train_loss: 0.22320737\n","Batch  1700\n","train_loss: 1.43821001\n","Batch  1800\n","train_loss: 1.28587437\n","Batch  1900\n","train_loss: 0.46943104\n","Batch  2000\n","train_loss: 1.17793071\n","Batch  2100\n","train_loss: 0.62565893\n","Epoch: 22 | train_loss:0.62572881\n","Batch  0\n","val_loss: 4.71954536\n","Batch  100\n","val_loss: 6.42918730\n","Batch  200\n","val_loss: 6.16277933\n","Batch  300\n","val_loss: 8.30155277\n","Epoch: 22 | val_loss:6.79584954\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 23 | Time: 6m 8s ***********\n","Batch  0\n","train_loss: 0.12595800\n","Batch  100\n","train_loss: 0.80460221\n","Batch  200\n","train_loss: 0.69855702\n","Batch  300\n","train_loss: 0.82961524\n","Batch  400\n","train_loss: 0.65734994\n","Batch  500\n","train_loss: 0.21300641\n","Batch  600\n","train_loss: 0.62235987\n","Batch  700\n","train_loss: 0.67508584\n","Batch  800\n","train_loss: 0.79774046\n","Batch  900\n","train_loss: 0.37045026\n","Batch  1000\n","train_loss: 0.05903322\n","Batch  1100\n","train_loss: 0.27983782\n","Batch  1200\n","train_loss: 1.03309095\n","Batch  1300\n","train_loss: 1.02628195\n","Batch  1400\n","train_loss: 0.46858323\n","Batch  1500\n","train_loss: 0.40747410\n","Batch  1600\n","train_loss: 1.18789089\n","Batch  1700\n","train_loss: 1.24628580\n","Batch  1800\n","train_loss: 0.45491275\n","Batch  1900\n","train_loss: 1.65355980\n","Batch  2000\n","train_loss: 1.04264617\n","Batch  2100\n","train_loss: 0.52182454\n","Epoch: 23 | train_loss:0.62271817\n","Batch  0\n","val_loss: 4.83475304\n","Batch  100\n","val_loss: 6.38963890\n","Batch  200\n","val_loss: 6.17108345\n","Batch  300\n","val_loss: 8.44145012\n","Epoch: 23 | val_loss:6.72148326\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 24 | Time: 6m 8s ***********\n","Batch  0\n","train_loss: 0.03816454\n","Batch  100\n","train_loss: 0.62243026\n","Batch  200\n","train_loss: 0.36544505\n","Batch  300\n","train_loss: 0.49457696\n","Batch  400\n","train_loss: 0.94096321\n","Batch  500\n","train_loss: 0.29649428\n","Batch  600\n","train_loss: 0.16017282\n","Batch  700\n","train_loss: 0.70808905\n","Batch  800\n","train_loss: 0.76926684\n","Batch  900\n","train_loss: 0.19907703\n","Batch  1000\n","train_loss: 0.73895168\n","Batch  1100\n","train_loss: 0.30010679\n","Batch  1200\n","train_loss: 0.34207806\n","Batch  1300\n","train_loss: 0.54702830\n","Batch  1400\n","train_loss: 0.66940463\n","Batch  1500\n","train_loss: 0.52003670\n","Batch  1600\n","train_loss: 0.56382400\n","Batch  1700\n","train_loss: 0.53919154\n","Batch  1800\n","train_loss: 0.56437063\n","Batch  1900\n","train_loss: 0.53890812\n","Batch  2000\n","train_loss: 0.62717599\n","Batch  2100\n","train_loss: 0.66871744\n","Epoch: 24 | train_loss:0.62584147\n","Batch  0\n","val_loss: 4.47502279\n","Batch  100\n","val_loss: 5.73902130\n","Batch  200\n","val_loss: 6.07710791\n","Batch  300\n","val_loss: 8.30548191\n","Epoch: 24 | val_loss:6.71125206\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 25 | Time: 6m 8s ***********\n","Batch  0\n","train_loss: 0.48586479\n","Batch  100\n","train_loss: 0.47561017\n","Batch  200\n","train_loss: 0.31916979\n","Batch  300\n","train_loss: 0.62607384\n","Batch  400\n","train_loss: 0.63982862\n","Batch  500\n","train_loss: 0.78970152\n","Batch  600\n","train_loss: 0.04686021\n","Batch  700\n","train_loss: 0.36671594\n","Batch  800\n","train_loss: 1.25341403\n","Batch  900\n","train_loss: 1.15848899\n","Batch  1000\n","train_loss: 0.99433738\n","Batch  1100\n","train_loss: 0.84414500\n","Batch  1200\n","train_loss: 0.77499092\n","Batch  1300\n","train_loss: 0.26067004\n","Batch  1400\n","train_loss: 0.71847212\n","Batch  1500\n","train_loss: 0.57507813\n","Batch  1600\n","train_loss: 0.76084673\n","Batch  1700\n","train_loss: 0.05263729\n","Batch  1800\n","train_loss: 1.14871800\n","Batch  1900\n","train_loss: 1.04806769\n","Batch  2000\n","train_loss: 1.81689429\n","Batch  2100\n","train_loss: 0.45427528\n","Epoch: 25 | train_loss:0.63494941\n","Batch  0\n","val_loss: 4.39625072\n","Batch  100\n","val_loss: 5.75611544\n","Batch  200\n","val_loss: 6.20742798\n","Batch  300\n","val_loss: 8.04331875\n","Epoch: 25 | val_loss:6.70776507\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 26 | Time: 6m 7s ***********\n","Batch  0\n","train_loss: 0.36118922\n","Batch  100\n","train_loss: 0.40601212\n","Batch  200\n","train_loss: 0.35353816\n","Batch  300\n","train_loss: 0.35911986\n","Batch  400\n","train_loss: 0.06345855\n","Batch  500\n","train_loss: 0.53128248\n","Batch  600\n","train_loss: 0.77079505\n","Batch  700\n","train_loss: 1.11213958\n","Batch  800\n","train_loss: 0.56715995\n","Batch  900\n","train_loss: 0.34229353\n","Batch  1000\n","train_loss: 0.37231374\n","Batch  1100\n","train_loss: 0.90580153\n","Batch  1200\n","train_loss: 0.87596977\n","Batch  1300\n","train_loss: 1.04247653\n","Batch  1400\n","train_loss: 0.66588038\n","Batch  1500\n","train_loss: 0.80501521\n","Batch  1600\n","train_loss: 1.26165307\n","Batch  1700\n","train_loss: 0.55522430\n","Batch  1800\n","train_loss: 0.35646155\n","Batch  1900\n","train_loss: 0.28169942\n","Batch  2000\n","train_loss: 0.90908456\n","Batch  2100\n","train_loss: 0.16550525\n","Epoch: 26 | train_loss:0.64073589\n","Batch  0\n","val_loss: 4.48572683\n","Batch  100\n","val_loss: 6.35255957\n","Batch  200\n","val_loss: 6.76468515\n","Batch  300\n","val_loss: 7.93499184\n","Epoch: 26 | val_loss:6.84054139\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 27 | Time: 6m 8s ***********\n","Batch  0\n","train_loss: 0.30042785\n","Batch  100\n","train_loss: 0.17063285\n","Batch  200\n","train_loss: 0.11710624\n","Batch  300\n","train_loss: 0.39882985\n","Batch  400\n","train_loss: 0.63440132\n","Batch  500\n","train_loss: 0.39220637\n","Batch  600\n","train_loss: 0.78744382\n","Batch  700\n","train_loss: 0.86965436\n","Batch  800\n","train_loss: 1.43232667\n","Batch  900\n","train_loss: 0.96223974\n","Batch  1000\n","train_loss: 0.65128762\n","Batch  1100\n","train_loss: 0.22800918\n","Batch  1200\n","train_loss: 0.22088116\n","Batch  1300\n","train_loss: 2.06975603\n","Batch  1400\n","train_loss: 0.61762899\n","Batch  1500\n","train_loss: 0.40006125\n","Batch  1600\n","train_loss: 0.74332345\n","Batch  1700\n","train_loss: 0.76339841\n","Batch  1800\n","train_loss: 0.28242692\n","Batch  1900\n","train_loss: 0.72987962\n","Batch  2000\n","train_loss: 0.85310876\n","Batch  2100\n","train_loss: 0.81596512\n","Epoch: 27 | train_loss:0.62986283\n","Batch  0\n","val_loss: 4.15659094\n","Batch  100\n","val_loss: 5.18400621\n","Batch  200\n","val_loss: 6.64885712\n","Batch  300\n","val_loss: 8.98932457\n","Epoch: 27 | val_loss:6.81812146\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 28 | Time: 6m 7s ***********\n","Batch  0\n","train_loss: 0.66639346\n","Batch  100\n","train_loss: 0.29949650\n","Batch  200\n","train_loss: 1.39314699\n","Batch  300\n","train_loss: 0.42687538\n","Batch  400\n","train_loss: 0.59074020\n","Batch  500\n","train_loss: 0.43216106\n","Batch  600\n","train_loss: 0.83977103\n","Batch  700\n","train_loss: 0.48338509\n","Batch  800\n","train_loss: 1.07427442\n","Batch  900\n","train_loss: 1.06708992\n","Batch  1000\n","train_loss: 0.48707870\n","Batch  1100\n","train_loss: 1.16806805\n","Batch  1200\n","train_loss: 0.26309988\n","Batch  1300\n","train_loss: 0.48270604\n","Batch  1400\n","train_loss: 0.11428456\n","Batch  1500\n","train_loss: 0.75979084\n","Batch  1600\n","train_loss: 0.19995773\n","Batch  1700\n","train_loss: 0.49630100\n","Batch  1800\n","train_loss: 0.71121079\n","Batch  1900\n","train_loss: 0.45829150\n","Batch  2000\n","train_loss: 0.62444001\n","Batch  2100\n","train_loss: 0.79870141\n","Epoch: 28 | train_loss:0.63441366\n","Batch  0\n","val_loss: 4.17901993\n","Batch  100\n","val_loss: 5.49303722\n","Batch  200\n","val_loss: 6.35206413\n","Batch  300\n","val_loss: 8.43530846\n","Epoch: 28 | val_loss:6.75867572\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 29 | Time: 6m 6s ***********\n","Batch  0\n","train_loss: 0.58525926\n","Batch  100\n","train_loss: 0.44362974\n","Batch  200\n","train_loss: 0.41460031\n","Batch  300\n","train_loss: 0.40246883\n","Batch  400\n","train_loss: 0.74630070\n","Batch  500\n","train_loss: 0.21035723\n","Batch  600\n","train_loss: 0.30885971\n","Batch  700\n","train_loss: 0.10074970\n","Batch  800\n","train_loss: 0.36081651\n","Batch  900\n","train_loss: 0.42752588\n","Batch  1000\n","train_loss: 0.65810901\n","Batch  1100\n","train_loss: 0.87277108\n","Batch  1200\n","train_loss: 0.48210970\n","Batch  1300\n","train_loss: 0.89893037\n","Batch  1400\n","train_loss: 0.49479824\n","Batch  1500\n","train_loss: 0.03519899\n","Batch  1600\n","train_loss: 0.41640419\n","Batch  1700\n","train_loss: 0.52960271\n","Batch  1800\n","train_loss: 1.34562016\n","Batch  1900\n","train_loss: 0.77837121\n","Batch  2000\n","train_loss: 0.72675377\n","Batch  2100\n","train_loss: 0.91196454\n","Epoch: 29 | train_loss:0.63510199\n","Batch  0\n","val_loss: 4.13595343\n","Batch  100\n","val_loss: 5.91000509\n","Batch  200\n","val_loss: 6.15298510\n","Batch  300\n","val_loss: 8.36304665\n","Epoch: 29 | val_loss:6.74562711\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 30 | Time: 6m 9s ***********\n","Batch  0\n","train_loss: 0.41326556\n","Batch  100\n","train_loss: 0.96587843\n","Batch  200\n","train_loss: 0.54472673\n","Batch  300\n","train_loss: 0.64887744\n","Batch  400\n","train_loss: 0.23685184\n","Batch  500\n","train_loss: 0.63828492\n","Batch  600\n","train_loss: 0.39059645\n","Batch  700\n","train_loss: 0.72194892\n","Batch  800\n","train_loss: 0.65795678\n","Batch  900\n","train_loss: 0.47427818\n","Batch  1000\n","train_loss: 1.15279388\n","Batch  1100\n","train_loss: 0.41859666\n","Batch  1200\n","train_loss: 0.57777828\n","Batch  1300\n","train_loss: 0.38827354\n","Batch  1400\n","train_loss: 0.87342513\n","Batch  1500\n","train_loss: 0.99151754\n","Batch  1600\n","train_loss: 1.43985987\n","Batch  1700\n","train_loss: 1.07707942\n","Batch  1800\n","train_loss: 0.58529234\n","Batch  1900\n","train_loss: 0.58494782\n","Batch  2000\n","train_loss: 0.64337468\n","Batch  2100\n","train_loss: 0.47076070\n","Epoch: 30 | train_loss:0.63684763\n","Batch  0\n","val_loss: 4.16171980\n","Batch  100\n","val_loss: 5.86672735\n","Batch  200\n","val_loss: 5.91376400\n","Batch  300\n","val_loss: 7.46530724\n","Epoch: 30 | val_loss:6.81912916\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 31 | Time: 6m 12s ***********\n","Batch  0\n","train_loss: 1.13471532\n","Batch  100\n","train_loss: 0.23310530\n","Batch  200\n","train_loss: 0.30197161\n","Batch  300\n","train_loss: 0.52015781\n","Batch  400\n","train_loss: 0.65043533\n","Batch  500\n","train_loss: 0.76699871\n","Batch  600\n","train_loss: 0.67215836\n","Batch  700\n","train_loss: 0.50320852\n","Batch  800\n","train_loss: 0.70094168\n","Batch  900\n","train_loss: 0.76290774\n","Batch  1000\n","train_loss: 0.76029217\n","Batch  1100\n","train_loss: 0.24019806\n","Batch  1200\n","train_loss: 0.36172876\n","Batch  1300\n","train_loss: 0.17042638\n","Batch  1400\n","train_loss: 0.94001353\n","Batch  1500\n","train_loss: 0.84131598\n","Batch  1600\n","train_loss: 0.42391399\n","Batch  1700\n","train_loss: 0.33113441\n","Batch  1800\n","train_loss: 0.85025370\n","Batch  1900\n","train_loss: 0.37314975\n","Batch  2000\n","train_loss: 1.31221759\n","Batch  2100\n","train_loss: 0.71813273\n","Epoch: 31 | train_loss:0.64512095\n","Batch  0\n","val_loss: 3.93145847\n","Batch  100\n","val_loss: 5.99907637\n","Batch  200\n","val_loss: 5.62241745\n","Batch  300\n","val_loss: 7.87318087\n","Epoch: 31 | val_loss:6.72249395\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 32 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.21090667\n","Batch  100\n","train_loss: 1.34174252\n","Batch  200\n","train_loss: 1.00761974\n","Batch  300\n","train_loss: 0.97765446\n","Batch  400\n","train_loss: 0.33367437\n","Batch  500\n","train_loss: 1.03771937\n","Batch  600\n","train_loss: 0.91765344\n","Batch  700\n","train_loss: 0.12789989\n","Batch  800\n","train_loss: 0.16595654\n","Batch  900\n","train_loss: 0.78789896\n","Batch  1000\n","train_loss: 0.55579376\n","Batch  1100\n","train_loss: 0.58174413\n","Batch  1200\n","train_loss: 0.78882688\n","Batch  1300\n","train_loss: 0.74572611\n","Batch  1400\n","train_loss: 0.34961382\n","Batch  1500\n","train_loss: 0.19663285\n","Batch  1600\n","train_loss: 0.97016376\n","Batch  1700\n","train_loss: 0.84074402\n","Batch  1800\n","train_loss: 0.25595415\n","Batch  1900\n","train_loss: 0.73203820\n","Batch  2000\n","train_loss: 0.94003391\n","Batch  2100\n","train_loss: 0.25719854\n","Epoch: 32 | train_loss:0.64762493\n","Batch  0\n","val_loss: 4.08459425\n","Batch  100\n","val_loss: 5.52893925\n","Batch  200\n","val_loss: 6.78352261\n","Batch  300\n","val_loss: 8.02393150\n","Epoch: 32 | val_loss:6.81178242\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 33 | Time: 6m 20s ***********\n","Batch  0\n","train_loss: 0.52847940\n","Batch  100\n","train_loss: 0.43289921\n","Batch  200\n","train_loss: 0.58808994\n","Batch  300\n","train_loss: 0.46878991\n","Batch  400\n","train_loss: 0.73465741\n","Batch  500\n","train_loss: 0.70688766\n","Batch  600\n","train_loss: 0.66565484\n","Batch  700\n","train_loss: 0.12172716\n","Batch  800\n","train_loss: 0.28650376\n","Batch  900\n","train_loss: 1.14982748\n","Batch  1000\n","train_loss: 0.44258851\n","Batch  1100\n","train_loss: 1.22098291\n","Batch  1200\n","train_loss: 1.15819001\n","Batch  1300\n","train_loss: 0.32221678\n","Batch  1400\n","train_loss: 0.39563969\n","Batch  1500\n","train_loss: 0.96604800\n","Batch  1600\n","train_loss: 0.49221689\n","Batch  1700\n","train_loss: 0.88948309\n","Batch  1800\n","train_loss: 1.55239296\n","Batch  1900\n","train_loss: 0.71845996\n","Batch  2000\n","train_loss: 0.68947417\n","Batch  2100\n","train_loss: 0.71838856\n","Epoch: 33 | train_loss:0.64364159\n","Batch  0\n","val_loss: 4.26554489\n","Batch  100\n","val_loss: 5.86896181\n","Batch  200\n","val_loss: 6.37593317\n","Batch  300\n","val_loss: 8.22998238\n","Epoch: 33 | val_loss:6.78056878\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 34 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.80264622\n","Batch  100\n","train_loss: 0.97339976\n","Batch  200\n","train_loss: 0.50036955\n","Batch  300\n","train_loss: 0.59073275\n","Batch  400\n","train_loss: 0.24150091\n","Batch  500\n","train_loss: 0.31333700\n","Batch  600\n","train_loss: 0.92566156\n","Batch  700\n","train_loss: 0.39720941\n","Batch  800\n","train_loss: 0.32966146\n","Batch  900\n","train_loss: 0.67204010\n","Batch  1000\n","train_loss: 0.60449326\n","Batch  1100\n","train_loss: 0.97686183\n","Batch  1200\n","train_loss: 0.31102231\n","Batch  1300\n","train_loss: 0.70103967\n","Batch  1400\n","train_loss: 0.76903099\n","Batch  1500\n","train_loss: 0.47631192\n","Batch  1600\n","train_loss: 1.15514719\n","Batch  1700\n","train_loss: 0.86447316\n","Batch  1800\n","train_loss: 0.46571112\n","Batch  1900\n","train_loss: 0.79685658\n","Batch  2000\n","train_loss: 0.65476108\n","Batch  2100\n","train_loss: 0.90553027\n","Epoch: 34 | train_loss:0.65301208\n","Batch  0\n","val_loss: 4.49723673\n","Batch  100\n","val_loss: 5.09678698\n","Batch  200\n","val_loss: 5.43480825\n","Batch  300\n","val_loss: 8.09712791\n","Epoch: 34 | val_loss:6.80239846\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 35 | Time: 6m 19s ***********\n","Batch  0\n","train_loss: 0.86960018\n","Batch  100\n","train_loss: 0.56897199\n","Batch  200\n","train_loss: 0.08546478\n","Batch  300\n","train_loss: 0.52496427\n","Batch  400\n","train_loss: 0.61930656\n","Batch  500\n","train_loss: 0.95525402\n","Batch  600\n","train_loss: 0.91389167\n","Batch  700\n","train_loss: 0.46251112\n","Batch  800\n","train_loss: 0.74343425\n","Batch  900\n","train_loss: 0.78125739\n","Batch  1000\n","train_loss: 0.57479256\n","Batch  1100\n","train_loss: 0.50796795\n","Batch  1200\n","train_loss: 0.31395507\n","Batch  1300\n","train_loss: 0.62322444\n","Batch  1400\n","train_loss: 0.86647189\n","Batch  1500\n","train_loss: 0.34562230\n","Batch  1600\n","train_loss: 1.30785751\n","Batch  1700\n","train_loss: 0.59564799\n","Batch  1800\n","train_loss: 0.46225861\n","Batch  1900\n","train_loss: 0.67793554\n","Batch  2000\n","train_loss: 1.11109841\n","Batch  2100\n","train_loss: 0.21785745\n","Epoch: 35 | train_loss:0.65334810\n","Batch  0\n","val_loss: 4.22125196\n","Batch  100\n","val_loss: 6.09585428\n","Batch  200\n","val_loss: 5.88329649\n","Batch  300\n","val_loss: 7.97297859\n","Epoch: 35 | val_loss:6.69512693\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 36 | Time: 6m 21s ***********\n","Batch  0\n","train_loss: 0.31612223\n","Batch  100\n","train_loss: 0.12261665\n","Batch  200\n","train_loss: 0.97168756\n","Batch  300\n","train_loss: 0.49363130\n","Batch  400\n","train_loss: 0.71582645\n","Batch  500\n","train_loss: 0.55617499\n","Batch  600\n","train_loss: 0.17563652\n","Batch  700\n","train_loss: 1.19694102\n","Batch  800\n","train_loss: 1.08597863\n","Batch  900\n","train_loss: 1.21832871\n","Batch  1000\n","train_loss: 0.76721901\n","Batch  1100\n","train_loss: 0.81263107\n","Batch  1200\n","train_loss: 0.64679581\n","Batch  1300\n","train_loss: 0.49786842\n","Batch  1400\n","train_loss: 0.76143336\n","Batch  1500\n","train_loss: 1.08344018\n","Batch  1600\n","train_loss: 0.58428073\n","Batch  1700\n","train_loss: 0.64884216\n","Batch  1800\n","train_loss: 0.50675112\n","Batch  1900\n","train_loss: 0.48088461\n","Batch  2000\n","train_loss: 1.57986128\n","Batch  2100\n","train_loss: 0.24155276\n","Epoch: 36 | train_loss:0.65704468\n","Batch  0\n","val_loss: 4.54807377\n","Batch  100\n","val_loss: 5.87358332\n","Batch  200\n","val_loss: 6.72469950\n","Batch  300\n","val_loss: 8.39046478\n","Epoch: 36 | val_loss:6.76913224\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 37 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 1.60765696\n","Batch  100\n","train_loss: 0.58630913\n","Batch  200\n","train_loss: 0.49987942\n","Batch  300\n","train_loss: 0.32893804\n","Batch  400\n","train_loss: 0.34903258\n","Batch  500\n","train_loss: 0.62184870\n","Batch  600\n","train_loss: 0.82026577\n","Batch  700\n","train_loss: 0.12586300\n","Batch  800\n","train_loss: 0.82153577\n","Batch  900\n","train_loss: 0.65643138\n","Batch  1000\n","train_loss: 0.41569656\n","Batch  1100\n","train_loss: 0.62230206\n","Batch  1200\n","train_loss: 0.93446904\n","Batch  1300\n","train_loss: 0.84081519\n","Batch  1400\n","train_loss: 0.78587407\n","Batch  1500\n","train_loss: 0.25989154\n","Batch  1600\n","train_loss: 1.24082935\n","Batch  1700\n","train_loss: 0.87312299\n","Batch  1800\n","train_loss: 1.14846504\n","Batch  1900\n","train_loss: 0.72115338\n","Batch  2000\n","train_loss: 1.18856561\n","Batch  2100\n","train_loss: 0.07976960\n","Epoch: 37 | train_loss:0.66757848\n","Batch  0\n","val_loss: 4.12854958\n","Batch  100\n","val_loss: 6.77856207\n","Batch  200\n","val_loss: 6.22010422\n","Batch  300\n","val_loss: 8.51953793\n","Epoch: 37 | val_loss:6.80440374\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 38 | Time: 6m 27s ***********\n","Batch  0\n","train_loss: 0.43077150\n","Batch  100\n","train_loss: 0.46299857\n","Batch  200\n","train_loss: 0.81422913\n","Batch  300\n","train_loss: 1.01027274\n","Batch  400\n","train_loss: 0.18275346\n","Batch  500\n","train_loss: 0.55986881\n","Batch  600\n","train_loss: 0.81352586\n","Batch  700\n","train_loss: 0.12742460\n","Batch  800\n","train_loss: 1.12481892\n","Batch  900\n","train_loss: 0.86893851\n","Batch  1000\n","train_loss: 0.53250128\n","Batch  1100\n","train_loss: 0.59185421\n","Batch  1200\n","train_loss: 0.46006238\n","Batch  1300\n","train_loss: 0.47040871\n","Batch  1400\n","train_loss: 0.47421548\n","Batch  1500\n","train_loss: 0.75291777\n","Batch  1600\n","train_loss: 0.04362549\n","Batch  1700\n","train_loss: 0.86891377\n","Batch  1800\n","train_loss: 0.23036794\n","Batch  1900\n","train_loss: 1.23325717\n","Batch  2000\n","train_loss: 1.04280365\n","Batch  2100\n","train_loss: 0.57934874\n","Epoch: 38 | train_loss:0.65394443\n","Batch  0\n","val_loss: 4.22365284\n","Batch  100\n","val_loss: 5.98427916\n","Batch  200\n","val_loss: 6.58564854\n","Batch  300\n","val_loss: 8.07743168\n","Epoch: 38 | val_loss:6.71186607\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 39 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.49895409\n","Batch  100\n","train_loss: 0.39386785\n","Batch  200\n","train_loss: 0.02934200\n","Batch  300\n","train_loss: 0.51201332\n","Batch  400\n","train_loss: 0.51255822\n","Batch  500\n","train_loss: 0.53495038\n","Batch  600\n","train_loss: 1.03534138\n","Batch  700\n","train_loss: 0.38920105\n","Batch  800\n","train_loss: 0.59339070\n","Batch  900\n","train_loss: 0.79441309\n","Batch  1000\n","train_loss: 0.17815296\n","Batch  1100\n","train_loss: 0.60873032\n","Batch  1200\n","train_loss: 0.36873364\n","Batch  1300\n","train_loss: 0.30331245\n","Batch  1400\n","train_loss: 0.18593307\n","Batch  1500\n","train_loss: 0.32686457\n","Batch  1600\n","train_loss: 0.58390850\n","Batch  1700\n","train_loss: 0.78889281\n","Batch  1800\n","train_loss: 0.64393651\n","Batch  1900\n","train_loss: 0.93101841\n","Batch  2000\n","train_loss: 0.28768262\n","Batch  2100\n","train_loss: 0.77192938\n","Epoch: 39 | train_loss:0.66065514\n","Batch  0\n","val_loss: 4.22941780\n","Batch  100\n","val_loss: 6.20890856\n","Batch  200\n","val_loss: 6.42280006\n","Batch  300\n","val_loss: 8.68608570\n","Epoch: 39 | val_loss:6.85883302\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 40 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.32094368\n","Batch  100\n","train_loss: 0.23603609\n","Batch  200\n","train_loss: 0.94212824\n","Batch  300\n","train_loss: 0.79018980\n","Batch  400\n","train_loss: 0.78896433\n","Batch  500\n","train_loss: 0.52242267\n","Batch  600\n","train_loss: 0.61642033\n","Batch  700\n","train_loss: 0.73469907\n","Batch  800\n","train_loss: 0.67771012\n","Batch  900\n","train_loss: 0.38995191\n","Batch  1000\n","train_loss: 0.85603559\n","Batch  1100\n","train_loss: 0.94528627\n","Batch  1200\n","train_loss: 0.76583755\n","Batch  1300\n","train_loss: 1.25277936\n","Batch  1400\n","train_loss: 0.55258965\n","Batch  1500\n","train_loss: 1.34466851\n","Batch  1600\n","train_loss: 0.65687221\n","Batch  1700\n","train_loss: 0.62687176\n","Batch  1800\n","train_loss: 1.03529179\n","Batch  1900\n","train_loss: 0.62532026\n","Batch  2000\n","train_loss: 0.78247583\n","Batch  2100\n","train_loss: 0.24652112\n","Epoch: 40 | train_loss:0.65836900\n","Batch  0\n","val_loss: 4.17506695\n","Batch  100\n","val_loss: 6.36365604\n","Batch  200\n","val_loss: 6.73589277\n","Batch  300\n","val_loss: 8.09708786\n","Epoch: 40 | val_loss:6.74947888\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 41 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.57616818\n","Batch  100\n","train_loss: 0.99294710\n","Batch  200\n","train_loss: 0.33123806\n","Batch  300\n","train_loss: 0.56142145\n","Batch  400\n","train_loss: 0.56322551\n","Batch  500\n","train_loss: 0.65019840\n","Batch  600\n","train_loss: 0.52015209\n","Batch  700\n","train_loss: 0.63472199\n","Batch  800\n","train_loss: 0.93670207\n","Batch  900\n","train_loss: 0.33479047\n","Batch  1000\n","train_loss: 0.41056171\n","Batch  1100\n","train_loss: 0.80561537\n","Batch  1200\n","train_loss: 1.54339778\n","Batch  1300\n","train_loss: 0.28527465\n","Batch  1400\n","train_loss: 0.90480220\n","Batch  1500\n","train_loss: 0.20522220\n","Batch  1600\n","train_loss: 0.92231524\n","Batch  1700\n","train_loss: 1.02540219\n","Batch  1800\n","train_loss: 0.62438703\n","Batch  1900\n","train_loss: 0.93955588\n","Batch  2000\n","train_loss: 0.58379221\n","Batch  2100\n","train_loss: 0.85822260\n","Epoch: 41 | train_loss:0.65997202\n","Batch  0\n","val_loss: 4.01679611\n","Batch  100\n","val_loss: 6.35732079\n","Batch  200\n","val_loss: 6.62198544\n","Batch  300\n","val_loss: 8.31956100\n","Epoch: 41 | val_loss:6.76609503\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 42 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.78070414\n","Batch  100\n","train_loss: 1.00932252\n","Batch  200\n","train_loss: 0.43011343\n","Batch  300\n","train_loss: 0.16964160\n","Batch  400\n","train_loss: 0.88751894\n","Batch  500\n","train_loss: 0.72433513\n","Batch  600\n","train_loss: 0.56899351\n","Batch  700\n","train_loss: 1.17140269\n","Batch  800\n","train_loss: 0.68147635\n","Batch  900\n","train_loss: 0.36912167\n","Batch  1000\n","train_loss: 1.11600077\n","Batch  1100\n","train_loss: 0.19732065\n","Batch  1200\n","train_loss: 0.45780185\n","Batch  1300\n","train_loss: 0.94921964\n","Batch  1400\n","train_loss: 0.85880041\n","Batch  1500\n","train_loss: 0.55064434\n","Batch  1600\n","train_loss: 0.05972765\n","Batch  1700\n","train_loss: 0.46805820\n","Batch  1800\n","train_loss: 0.24889103\n","Batch  1900\n","train_loss: 0.47473302\n","Batch  2000\n","train_loss: 0.79920220\n","Batch  2100\n","train_loss: 1.17914724\n","Epoch: 42 | train_loss:0.66750052\n","Batch  0\n","val_loss: 4.40773296\n","Batch  100\n","val_loss: 6.34322548\n","Batch  200\n","val_loss: 6.63183117\n","Batch  300\n","val_loss: 7.78811359\n","Epoch: 42 | val_loss:6.77008140\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 43 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.72323096\n","Batch  100\n","train_loss: 0.60320371\n","Batch  200\n","train_loss: 0.30061191\n","Batch  300\n","train_loss: 0.36652637\n","Batch  400\n","train_loss: 0.68021864\n","Batch  500\n","train_loss: 0.69918185\n","Batch  600\n","train_loss: 0.62414092\n","Batch  700\n","train_loss: 0.80038720\n","Batch  800\n","train_loss: 0.57807136\n","Batch  900\n","train_loss: 0.24061194\n","Batch  1000\n","train_loss: 0.16450036\n","Batch  1100\n","train_loss: 1.41057813\n","Batch  1200\n","train_loss: 0.28587139\n","Batch  1300\n","train_loss: 0.40387169\n","Batch  1400\n","train_loss: 0.57483834\n","Batch  1500\n","train_loss: 0.27688339\n","Batch  1600\n","train_loss: 1.08615232\n","Batch  1700\n","train_loss: 1.25332689\n","Batch  1800\n","train_loss: 0.42408568\n","Batch  1900\n","train_loss: 0.78099829\n","Batch  2000\n","train_loss: 0.49258056\n","Batch  2100\n","train_loss: 0.67578477\n","Epoch: 43 | train_loss:0.67273487\n","Batch  0\n","val_loss: 4.41025305\n","Batch  100\n","val_loss: 6.13292599\n","Batch  200\n","val_loss: 5.74230957\n","Batch  300\n","val_loss: 8.37990665\n","Epoch: 43 | val_loss:6.75312224\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 44 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.45417356\n","Batch  100\n","train_loss: 0.26117241\n","Batch  200\n","train_loss: 0.30010107\n","Batch  300\n","train_loss: 1.96233380\n","Batch  400\n","train_loss: 0.38463387\n","Batch  500\n","train_loss: 0.09987248\n","Batch  600\n","train_loss: 0.90911460\n","Batch  700\n","train_loss: 0.84748119\n","Batch  800\n","train_loss: 0.73882461\n","Batch  900\n","train_loss: 0.66452974\n","Batch  1000\n","train_loss: 0.36824617\n","Batch  1100\n","train_loss: 0.90175158\n","Batch  1200\n","train_loss: 0.91328281\n","Batch  1300\n","train_loss: 0.30017811\n","Batch  1400\n","train_loss: 0.51724100\n","Batch  1500\n","train_loss: 0.61954045\n","Batch  1600\n","train_loss: 1.05426633\n","Batch  1700\n","train_loss: 0.89834118\n","Batch  1800\n","train_loss: 0.63539994\n","Batch  1900\n","train_loss: 0.47793746\n","Batch  2000\n","train_loss: 0.71895778\n","Batch  2100\n","train_loss: 0.56350827\n","Epoch: 44 | train_loss:0.67099808\n","Batch  0\n","val_loss: 4.32287693\n","Batch  100\n","val_loss: 5.83035517\n","Batch  200\n","val_loss: 6.10998392\n","Batch  300\n","val_loss: 8.04116249\n","Epoch: 44 | val_loss:6.76906382\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 45 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.78290254\n","Batch  100\n","train_loss: 0.55014223\n","Batch  200\n","train_loss: 0.17902622\n","Batch  300\n","train_loss: 0.71341699\n","Batch  400\n","train_loss: 0.15302305\n","Batch  500\n","train_loss: 0.25561160\n","Batch  600\n","train_loss: 1.01873219\n","Batch  700\n","train_loss: 0.68917674\n","Batch  800\n","train_loss: 0.30538934\n","Batch  900\n","train_loss: 0.35077110\n","Batch  1000\n","train_loss: 0.72066677\n","Batch  1100\n","train_loss: 1.18564999\n","Batch  1200\n","train_loss: 0.73975295\n","Batch  1300\n","train_loss: 1.07685578\n","Batch  1400\n","train_loss: 0.51607788\n","Batch  1500\n","train_loss: 0.37602213\n","Batch  1600\n","train_loss: 0.76736569\n","Batch  1700\n","train_loss: 0.06439061\n","Batch  1800\n","train_loss: 0.92967844\n","Batch  1900\n","train_loss: 0.60725790\n","Batch  2000\n","train_loss: 0.30954745\n","Batch  2100\n","train_loss: 0.41017911\n","Epoch: 45 | train_loss:0.67199337\n","Batch  0\n","val_loss: 4.11238623\n","Batch  100\n","val_loss: 6.56871080\n","Batch  200\n","val_loss: 6.67919111\n","Batch  300\n","val_loss: 8.48422050\n","Epoch: 45 | val_loss:6.85005908\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 46 | Time: 6m 21s ***********\n","Batch  0\n","train_loss: 0.59090555\n","Batch  100\n","train_loss: 0.62685186\n","Batch  200\n","train_loss: 1.53368437\n","Batch  300\n","train_loss: 1.07894397\n","Batch  400\n","train_loss: 1.28802848\n","Batch  500\n","train_loss: 0.97910148\n","Batch  600\n","train_loss: 0.35836437\n","Batch  700\n","train_loss: 0.70348221\n","Batch  800\n","train_loss: 0.24204086\n","Batch  900\n","train_loss: 0.52823001\n","Batch  1000\n","train_loss: 0.36973196\n","Batch  1100\n","train_loss: 0.46856052\n","Batch  1200\n","train_loss: 1.86912000\n","Batch  1300\n","train_loss: 0.40622360\n","Batch  1400\n","train_loss: 0.16954207\n","Batch  1500\n","train_loss: 0.72550213\n","Batch  1600\n","train_loss: 1.52134526\n","Batch  1700\n","train_loss: 0.66915661\n","Batch  1800\n","train_loss: 0.70578074\n","Batch  1900\n","train_loss: 0.90511721\n","Batch  2000\n","train_loss: 1.05657446\n","Batch  2100\n","train_loss: 0.64254677\n","Epoch: 46 | train_loss:0.66539838\n","Batch  0\n","val_loss: 4.29719543\n","Batch  100\n","val_loss: 5.88438320\n","Batch  200\n","val_loss: 6.20469475\n","Batch  300\n","val_loss: 8.16010189\n","Epoch: 46 | val_loss:6.83384438\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 47 | Time: 6m 19s ***********\n","Batch  0\n","train_loss: 0.40545794\n","Batch  100\n","train_loss: 0.99996620\n","Batch  200\n","train_loss: 0.90850884\n","Batch  300\n","train_loss: 0.59421748\n","Batch  400\n","train_loss: 1.04048491\n","Batch  500\n","train_loss: 0.79313135\n","Batch  600\n","train_loss: 0.50993264\n","Batch  700\n","train_loss: 0.56916028\n","Batch  800\n","train_loss: 0.68123078\n","Batch  900\n","train_loss: 1.03379381\n","Batch  1000\n","train_loss: 0.93014836\n","Batch  1100\n","train_loss: 0.30490276\n","Batch  1200\n","train_loss: 0.31250793\n","Batch  1300\n","train_loss: 0.02138799\n","Batch  1400\n","train_loss: 0.58921856\n","Batch  1500\n","train_loss: 0.93596077\n","Batch  1600\n","train_loss: 0.86712360\n","Batch  1700\n","train_loss: 0.54282069\n","Batch  1800\n","train_loss: 0.74294221\n","Batch  1900\n","train_loss: 0.06608543\n","Batch  2000\n","train_loss: 0.83751941\n","Batch  2100\n","train_loss: 0.30854782\n","Epoch: 47 | train_loss:0.68458652\n","Batch  0\n","val_loss: 4.28737783\n","Batch  100\n","val_loss: 5.23277235\n","Batch  200\n","val_loss: 6.13622427\n","Batch  300\n","val_loss: 8.38567162\n","Epoch: 47 | val_loss:6.74410545\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 48 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.96054089\n","Batch  100\n","train_loss: 0.51492459\n","Batch  200\n","train_loss: 0.57462370\n","Batch  300\n","train_loss: 0.54983300\n","Batch  400\n","train_loss: 1.07761240\n","Batch  500\n","train_loss: 0.17692009\n","Batch  600\n","train_loss: 0.65715367\n","Batch  700\n","train_loss: 1.02452755\n","Batch  800\n","train_loss: 0.88077378\n","Batch  900\n","train_loss: 0.60717410\n","Batch  1000\n","train_loss: 0.08840799\n","Batch  1100\n","train_loss: 1.11787605\n","Batch  1200\n","train_loss: 0.99039769\n","Batch  1300\n","train_loss: 1.23106194\n","Batch  1400\n","train_loss: 0.04024309\n","Batch  1500\n","train_loss: 0.72404838\n","Batch  1600\n","train_loss: 0.52708411\n","Batch  1700\n","train_loss: 1.17745256\n","Batch  1800\n","train_loss: 1.52608633\n","Batch  1900\n","train_loss: 0.52894163\n","Batch  2000\n","train_loss: 0.45119369\n","Batch  2100\n","train_loss: 0.41077191\n","Epoch: 48 | train_loss:0.67786244\n","Batch  0\n","val_loss: 3.98885703\n","Batch  100\n","val_loss: 5.80766821\n","Batch  200\n","val_loss: 6.38068962\n","Batch  300\n","val_loss: 8.09367943\n","Epoch: 48 | val_loss:6.82376456\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 49 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.71065515\n","Batch  100\n","train_loss: 0.42381328\n","Batch  200\n","train_loss: 0.43110213\n","Batch  300\n","train_loss: 0.73164904\n","Batch  400\n","train_loss: 0.16838156\n","Batch  500\n","train_loss: 0.38265929\n","Batch  600\n","train_loss: 0.27201468\n","Batch  700\n","train_loss: 0.90787512\n","Batch  800\n","train_loss: 0.52660638\n","Batch  900\n","train_loss: 1.23639119\n","Batch  1000\n","train_loss: 0.51032156\n","Batch  1100\n","train_loss: 0.18947743\n","Batch  1200\n","train_loss: 0.04621208\n","Batch  1300\n","train_loss: 0.75436449\n","Batch  1400\n","train_loss: 0.81536454\n","Batch  1500\n","train_loss: 1.07447183\n","Batch  1600\n","train_loss: 0.76234543\n","Batch  1700\n","train_loss: 0.92027622\n","Batch  1800\n","train_loss: 0.79665482\n","Batch  1900\n","train_loss: 0.71055621\n","Batch  2000\n","train_loss: 0.27706608\n","Batch  2100\n","train_loss: 0.59691840\n","Epoch: 49 | train_loss:0.68254939\n","Batch  0\n","val_loss: 4.34562111\n","Batch  100\n","val_loss: 6.04397821\n","Batch  200\n","val_loss: 6.36823845\n","Batch  300\n","val_loss: 8.29052639\n","Epoch: 49 | val_loss:6.73500387\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 50 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.95762235\n","Batch  100\n","train_loss: 0.80525398\n","Batch  200\n","train_loss: 0.86655432\n","Batch  300\n","train_loss: 0.27592713\n","Batch  400\n","train_loss: 0.86362833\n","Batch  500\n","train_loss: 0.89253539\n","Batch  600\n","train_loss: 0.45040694\n","Batch  700\n","train_loss: 0.13089813\n","Batch  800\n","train_loss: 0.81151134\n","Batch  900\n","train_loss: 1.24458432\n","Batch  1000\n","train_loss: 0.06388723\n","Batch  1100\n","train_loss: 1.39801180\n","Batch  1200\n","train_loss: 1.57107151\n","Batch  1300\n","train_loss: 0.14129701\n","Batch  1400\n","train_loss: 0.30221629\n","Batch  1500\n","train_loss: 1.03980196\n","Batch  1600\n","train_loss: 0.57289362\n","Batch  1700\n","train_loss: 1.36260939\n","Batch  1800\n","train_loss: 0.55914384\n","Batch  1900\n","train_loss: 0.31470245\n","Batch  2000\n","train_loss: 1.07365334\n","Batch  2100\n","train_loss: 0.73042119\n","Epoch: 50 | train_loss:0.67576280\n","Batch  0\n","val_loss: 4.12595034\n","Batch  100\n","val_loss: 5.82825756\n","Batch  200\n","val_loss: 6.32648993\n","Batch  300\n","val_loss: 8.10618591\n","Epoch: 50 | val_loss:6.77813952\n","scheduler adjust learning rate to  0.001\n","*************Epoch: 51 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 1.28937948\n","Batch  100\n","train_loss: 0.43020451\n","Batch  200\n","train_loss: 0.53582013\n","Batch  300\n","train_loss: 0.94965422\n","Batch  400\n","train_loss: 0.89441919\n","Batch  500\n","train_loss: 0.06927053\n","Batch  600\n","train_loss: 0.65979201\n","Batch  700\n","train_loss: 0.60527515\n","Batch  800\n","train_loss: 0.77696955\n","Batch  900\n","train_loss: 1.29015028\n","Batch  1000\n","train_loss: 0.73376119\n","Batch  1100\n","train_loss: 0.62217456\n","Batch  1200\n","train_loss: 0.73984224\n","Batch  1300\n","train_loss: 1.06100941\n","Batch  1400\n","train_loss: 0.67284697\n","Batch  1500\n","train_loss: 0.69329476\n","Batch  1600\n","train_loss: 0.55742902\n","Batch  1700\n","train_loss: 0.95775509\n","Batch  1800\n","train_loss: 0.32110241\n","Batch  1900\n","train_loss: 0.77817690\n","Batch  2000\n","train_loss: 1.05078840\n","Batch  2100\n","train_loss: 0.71828675\n","Epoch: 51 | train_loss:0.67689462\n","Batch  0\n","val_loss: 3.90690160\n","Batch  100\n","val_loss: 5.82374477\n","Batch  200\n","val_loss: 6.01769257\n","Batch  300\n","val_loss: 7.84765625\n","Epoch: 51 | val_loss:6.68237639\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 52 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.91080457\n","Batch  100\n","train_loss: 0.03067928\n","Batch  200\n","train_loss: 0.28877625\n","Batch  300\n","train_loss: 0.43161178\n","Batch  400\n","train_loss: 0.47393471\n","Batch  500\n","train_loss: 0.62592107\n","Batch  600\n","train_loss: 0.35054460\n","Batch  700\n","train_loss: 0.61536181\n","Batch  800\n","train_loss: 0.67095345\n","Batch  900\n","train_loss: 0.61818510\n","Batch  1000\n","train_loss: 0.44253007\n","Batch  1100\n","train_loss: 0.60478646\n","Batch  1200\n","train_loss: 0.46639869\n","Batch  1300\n","train_loss: 0.12681012\n","Batch  1400\n","train_loss: 0.55466628\n","Batch  1500\n","train_loss: 0.66085124\n","Batch  1600\n","train_loss: 0.11751794\n","Batch  1700\n","train_loss: 0.70090777\n","Batch  1800\n","train_loss: 0.26906693\n","Batch  1900\n","train_loss: 0.75076067\n","Batch  2000\n","train_loss: 0.74284995\n","Batch  2100\n","train_loss: 0.02748601\n","Epoch: 52 | train_loss:0.50009372\n","Batch  0\n","val_loss: 4.23811340\n","Batch  100\n","val_loss: 6.22421598\n","Batch  200\n","val_loss: 5.77559996\n","Batch  300\n","val_loss: 7.55109644\n","Epoch: 52 | val_loss:6.73483599\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 53 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.41102314\n","Batch  100\n","train_loss: 0.06613101\n","Batch  200\n","train_loss: 0.10946156\n","Batch  300\n","train_loss: 0.47508636\n","Batch  400\n","train_loss: 0.23495176\n","Batch  500\n","train_loss: 0.48552114\n","Batch  600\n","train_loss: 0.34185943\n","Batch  700\n","train_loss: 0.05332080\n","Batch  800\n","train_loss: 0.25333339\n","Batch  900\n","train_loss: 0.35558817\n","Batch  1000\n","train_loss: 0.00901177\n","Batch  1100\n","train_loss: 0.04274799\n","Batch  1200\n","train_loss: 0.27461866\n","Batch  1300\n","train_loss: 0.35543436\n","Batch  1400\n","train_loss: 0.44559455\n","Batch  1500\n","train_loss: 0.24369141\n","Batch  1600\n","train_loss: 0.75828338\n","Batch  1700\n","train_loss: 0.38137481\n","Batch  1800\n","train_loss: 0.00806825\n","Batch  1900\n","train_loss: 0.12518501\n","Batch  2000\n","train_loss: 0.36668000\n","Batch  2100\n","train_loss: 0.04620670\n","Epoch: 53 | train_loss:0.30022048\n","Batch  0\n","val_loss: 4.26436090\n","Batch  100\n","val_loss: 6.11196852\n","Batch  200\n","val_loss: 5.97766447\n","Batch  300\n","val_loss: 7.46592426\n","Epoch: 53 | val_loss:6.83163451\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 54 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.02084753\n","Batch  100\n","train_loss: 0.15607782\n","Batch  200\n","train_loss: 0.15434061\n","Batch  300\n","train_loss: 0.63785368\n","Batch  400\n","train_loss: 0.29766750\n","Batch  500\n","train_loss: 0.29032683\n","Batch  600\n","train_loss: 0.12272915\n","Batch  700\n","train_loss: 0.31390318\n","Batch  800\n","train_loss: 0.11358108\n","Batch  900\n","train_loss: 0.06662992\n","Batch  1000\n","train_loss: 0.17315383\n","Batch  1100\n","train_loss: 0.41646481\n","Batch  1200\n","train_loss: 0.22293271\n","Batch  1300\n","train_loss: 0.05616435\n","Batch  1400\n","train_loss: 0.13732189\n","Batch  1500\n","train_loss: 0.30250123\n","Batch  1600\n","train_loss: 0.01346369\n","Batch  1700\n","train_loss: 0.19715561\n","Batch  1800\n","train_loss: 0.16887046\n","Batch  1900\n","train_loss: 0.31518951\n","Batch  2000\n","train_loss: 0.04558806\n","Batch  2100\n","train_loss: 0.03208058\n","Epoch: 54 | train_loss:0.19568316\n","Batch  0\n","val_loss: 4.31138659\n","Batch  100\n","val_loss: 6.29280615\n","Batch  200\n","val_loss: 6.50096226\n","Batch  300\n","val_loss: 7.76128912\n","Epoch: 54 | val_loss:6.92376329\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 55 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.01584079\n","Batch  100\n","train_loss: 0.11723919\n","Batch  200\n","train_loss: 0.07779786\n","Batch  300\n","train_loss: 0.00734191\n","Batch  400\n","train_loss: 0.14609538\n","Batch  500\n","train_loss: 0.07597652\n","Batch  600\n","train_loss: 0.09982889\n","Batch  700\n","train_loss: 0.20435950\n","Batch  800\n","train_loss: 0.12236863\n","Batch  900\n","train_loss: 0.42854252\n","Batch  1000\n","train_loss: 0.03521759\n","Batch  1100\n","train_loss: 0.26690799\n","Batch  1200\n","train_loss: 0.00573717\n","Batch  1300\n","train_loss: 0.16145983\n","Batch  1400\n","train_loss: 0.09266821\n","Batch  1500\n","train_loss: 0.05938542\n","Batch  1600\n","train_loss: 0.17387834\n","Batch  1700\n","train_loss: 0.43838903\n","Batch  1800\n","train_loss: 0.11641080\n","Batch  1900\n","train_loss: 0.09182937\n","Batch  2000\n","train_loss: 0.08022301\n","Batch  2100\n","train_loss: 0.39209586\n","Epoch: 55 | train_loss:0.13456370\n","Batch  0\n","val_loss: 4.36689854\n","Batch  100\n","val_loss: 6.52562809\n","Batch  200\n","val_loss: 6.79020166\n","Batch  300\n","val_loss: 8.05747795\n","Epoch: 55 | val_loss:7.02861023\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 56 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.06259827\n","Batch  100\n","train_loss: 0.11866809\n","Batch  200\n","train_loss: 0.09577494\n","Batch  300\n","train_loss: 0.07439699\n","Batch  400\n","train_loss: 0.09604167\n","Batch  500\n","train_loss: 0.10442559\n","Batch  600\n","train_loss: 0.16419592\n","Batch  700\n","train_loss: 0.06838998\n","Batch  800\n","train_loss: 0.14081080\n","Batch  900\n","train_loss: 0.03801407\n","Batch  1000\n","train_loss: 0.02195089\n","Batch  1100\n","train_loss: 0.01897955\n","Batch  1200\n","train_loss: 0.00924247\n","Batch  1300\n","train_loss: 0.03754942\n","Batch  1400\n","train_loss: 0.03586901\n","Batch  1500\n","train_loss: 0.02903949\n","Batch  1600\n","train_loss: 0.16873358\n","Batch  1700\n","train_loss: 0.07485905\n","Batch  1800\n","train_loss: 0.32654333\n","Batch  1900\n","train_loss: 0.05205110\n","Batch  2000\n","train_loss: 0.00609443\n","Batch  2100\n","train_loss: 0.06697772\n","Epoch: 56 | train_loss:0.09676316\n","Batch  0\n","val_loss: 4.34762716\n","Batch  100\n","val_loss: 7.08945704\n","Batch  200\n","val_loss: 6.85234308\n","Batch  300\n","val_loss: 7.96039391\n","Epoch: 56 | val_loss:7.14682732\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 57 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.04651970\n","Batch  100\n","train_loss: 0.02296027\n","Batch  200\n","train_loss: 0.02831576\n","Batch  300\n","train_loss: 0.05712612\n","Batch  400\n","train_loss: 0.00175166\n","Batch  500\n","train_loss: 0.05483551\n","Batch  600\n","train_loss: 0.01183666\n","Batch  700\n","train_loss: 0.00774110\n","Batch  800\n","train_loss: 0.07970629\n","Batch  900\n","train_loss: 0.00902264\n","Batch  1000\n","train_loss: 0.17680047\n","Batch  1100\n","train_loss: 0.10654239\n","Batch  1200\n","train_loss: 0.10927811\n","Batch  1300\n","train_loss: 0.13773201\n","Batch  1400\n","train_loss: 0.00758177\n","Batch  1500\n","train_loss: 0.12698907\n","Batch  1600\n","train_loss: 0.02671713\n","Batch  1700\n","train_loss: 0.09725317\n","Batch  1800\n","train_loss: 0.03516703\n","Batch  1900\n","train_loss: 0.02131607\n","Batch  2000\n","train_loss: 0.04270920\n","Batch  2100\n","train_loss: 0.02541081\n","Epoch: 57 | train_loss:0.06985975\n","Batch  0\n","val_loss: 4.42000294\n","Batch  100\n","val_loss: 7.23974943\n","Batch  200\n","val_loss: 6.19639206\n","Batch  300\n","val_loss: 8.57122707\n","Epoch: 57 | val_loss:7.30163438\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 58 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.02655604\n","Batch  100\n","train_loss: 0.04323331\n","Batch  200\n","train_loss: 0.02865507\n","Batch  300\n","train_loss: 0.01960901\n","Batch  400\n","train_loss: 0.01617881\n","Batch  500\n","train_loss: 0.04772907\n","Batch  600\n","train_loss: 0.02842259\n","Batch  700\n","train_loss: 0.10174824\n","Batch  800\n","train_loss: 0.03797000\n","Batch  900\n","train_loss: 0.00585028\n","Batch  1000\n","train_loss: 0.02370440\n","Batch  1100\n","train_loss: 0.00812507\n","Batch  1200\n","train_loss: 0.05185025\n","Batch  1300\n","train_loss: 0.00359344\n","Batch  1400\n","train_loss: 0.03894529\n","Batch  1500\n","train_loss: 0.02980572\n","Batch  1600\n","train_loss: 0.26054034\n","Batch  1700\n","train_loss: 0.30927557\n","Batch  1800\n","train_loss: 0.11066622\n","Batch  1900\n","train_loss: 0.01632987\n","Batch  2000\n","train_loss: 0.04635967\n","Batch  2100\n","train_loss: 0.01749755\n","Epoch: 58 | train_loss:0.05278180\n","Batch  0\n","val_loss: 4.44065046\n","Batch  100\n","val_loss: 7.39710903\n","Batch  200\n","val_loss: 7.12532616\n","Batch  300\n","val_loss: 8.73536491\n","Epoch: 58 | val_loss:7.40638432\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 59 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.01580237\n","Batch  100\n","train_loss: 0.00751807\n","Batch  200\n","train_loss: 0.02032033\n","Batch  300\n","train_loss: 0.07295036\n","Batch  400\n","train_loss: 0.02564166\n","Batch  500\n","train_loss: 0.02658162\n","Batch  600\n","train_loss: 0.05032037\n","Batch  700\n","train_loss: 0.01292863\n","Batch  800\n","train_loss: 0.20634848\n","Batch  900\n","train_loss: 0.03050119\n","Batch  1000\n","train_loss: 0.00782805\n","Batch  1100\n","train_loss: 0.00486838\n","Batch  1200\n","train_loss: 0.04013544\n","Batch  1300\n","train_loss: 0.05193897\n","Batch  1400\n","train_loss: 0.02983352\n","Batch  1500\n","train_loss: 0.01104601\n","Batch  1600\n","train_loss: 0.01549930\n","Batch  1700\n","train_loss: 0.01435750\n","Batch  1800\n","train_loss: 0.02101575\n","Batch  1900\n","train_loss: 0.01822324\n","Batch  2000\n","train_loss: 0.03027396\n","Batch  2100\n","train_loss: 0.03900921\n","Epoch: 59 | train_loss:0.04188286\n","Batch  0\n","val_loss: 4.41844511\n","Batch  100\n","val_loss: 7.79918575\n","Batch  200\n","val_loss: 6.27530241\n","Batch  300\n","val_loss: 8.50070381\n","Epoch: 59 | val_loss:7.51016061\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 60 | Time: 6m 19s ***********\n","Batch  0\n","train_loss: 0.00617919\n","Batch  100\n","train_loss: 0.06936184\n","Batch  200\n","train_loss: 0.00691745\n","Batch  300\n","train_loss: 0.00203430\n","Batch  400\n","train_loss: 0.01066643\n","Batch  500\n","train_loss: 0.02832061\n","Batch  600\n","train_loss: 0.07239259\n","Batch  700\n","train_loss: 0.02307245\n","Batch  800\n","train_loss: 0.10743514\n","Batch  900\n","train_loss: 0.02244614\n","Batch  1000\n","train_loss: 0.02417052\n","Batch  1100\n","train_loss: 0.00656661\n","Batch  1200\n","train_loss: 0.03213305\n","Batch  1300\n","train_loss: 0.02822126\n","Batch  1400\n","train_loss: 0.19270904\n","Batch  1500\n","train_loss: 0.01366592\n","Batch  1600\n","train_loss: 0.01740429\n","Batch  1700\n","train_loss: 0.02484796\n","Batch  1800\n","train_loss: 0.03032187\n","Batch  1900\n","train_loss: 0.04071052\n","Batch  2000\n","train_loss: 0.20083667\n","Batch  2100\n","train_loss: 0.00460693\n","Epoch: 60 | train_loss:0.03176716\n","Batch  0\n","val_loss: 4.35833693\n","Batch  100\n","val_loss: 7.73320484\n","Batch  200\n","val_loss: 6.94669867\n","Batch  300\n","val_loss: 9.16293907\n","Epoch: 60 | val_loss:7.66713238\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 61 | Time: 6m 20s ***********\n","Batch  0\n","train_loss: 0.01541495\n","Batch  100\n","train_loss: 0.02258992\n","Batch  200\n","train_loss: 0.00461341\n","Batch  300\n","train_loss: 0.00897500\n","Batch  400\n","train_loss: 0.02484512\n","Batch  500\n","train_loss: 0.02324253\n","Batch  600\n","train_loss: 0.01778534\n","Batch  700\n","train_loss: 0.00074063\n","Batch  800\n","train_loss: 0.00221805\n","Batch  900\n","train_loss: 0.00954423\n","Batch  1000\n","train_loss: 0.00507518\n","Batch  1100\n","train_loss: 0.01234088\n","Batch  1200\n","train_loss: 0.00624761\n","Batch  1300\n","train_loss: 0.01319232\n","Batch  1400\n","train_loss: 0.00146566\n","Batch  1500\n","train_loss: 0.00469120\n","Batch  1600\n","train_loss: 0.00236185\n","Batch  1700\n","train_loss: 0.01418187\n","Batch  1800\n","train_loss: 0.01626733\n","Batch  1900\n","train_loss: 0.00476091\n","Batch  2000\n","train_loss: 0.00843146\n","Batch  2100\n","train_loss: 0.00161643\n","Epoch: 61 | train_loss:0.02417460\n","Batch  0\n","val_loss: 4.79382706\n","Batch  100\n","val_loss: 7.51009512\n","Batch  200\n","val_loss: 7.38448095\n","Batch  300\n","val_loss: 8.90719223\n","Epoch: 61 | val_loss:7.75213807\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 62 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.00623776\n","Batch  100\n","train_loss: 0.02653706\n","Batch  200\n","train_loss: 0.00801618\n","Batch  300\n","train_loss: 0.02070493\n","Batch  400\n","train_loss: 0.04587481\n","Batch  500\n","train_loss: 0.00607322\n","Batch  600\n","train_loss: 0.01010568\n","Batch  700\n","train_loss: 0.17071001\n","Batch  800\n","train_loss: 0.00561663\n","Batch  900\n","train_loss: 0.01405490\n","Batch  1000\n","train_loss: 0.00650917\n","Batch  1100\n","train_loss: 0.01844261\n","Batch  1200\n","train_loss: 0.01400343\n","Batch  1300\n","train_loss: 0.00898097\n","Batch  1400\n","train_loss: 0.02467134\n","Batch  1500\n","train_loss: 0.02262046\n","Batch  1600\n","train_loss: 0.00504436\n","Batch  1700\n","train_loss: 0.00853487\n","Batch  1800\n","train_loss: 0.01264369\n","Batch  1900\n","train_loss: 0.02828973\n","Batch  2000\n","train_loss: 0.02334591\n","Batch  2100\n","train_loss: 0.00140613\n","Epoch: 62 | train_loss:0.02064035\n","Batch  0\n","val_loss: 4.63828754\n","Batch  100\n","val_loss: 7.68236351\n","Batch  200\n","val_loss: 7.09459162\n","Batch  300\n","val_loss: 9.61552525\n","Epoch: 62 | val_loss:7.88312253\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 63 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.01295424\n","Batch  100\n","train_loss: 0.00195159\n","Batch  200\n","train_loss: 0.00804870\n","Batch  300\n","train_loss: 0.00624576\n","Batch  400\n","train_loss: 0.00774707\n","Batch  500\n","train_loss: 0.00186682\n","Batch  600\n","train_loss: 0.01976631\n","Batch  700\n","train_loss: 0.00092846\n","Batch  800\n","train_loss: 0.02616990\n","Batch  900\n","train_loss: 0.01246161\n","Batch  1000\n","train_loss: 0.00740914\n","Batch  1100\n","train_loss: 0.07410014\n","Batch  1200\n","train_loss: 0.01510710\n","Batch  1300\n","train_loss: 0.00379869\n","Batch  1400\n","train_loss: 0.00743565\n","Batch  1500\n","train_loss: 0.02084335\n","Batch  1600\n","train_loss: 0.00634013\n","Batch  1700\n","train_loss: 0.00505953\n","Batch  1800\n","train_loss: 0.01293240\n","Batch  1900\n","train_loss: 0.01838593\n","Batch  2000\n","train_loss: 0.00111179\n","Batch  2100\n","train_loss: 0.00764558\n","Epoch: 63 | train_loss:0.01577143\n","Batch  0\n","val_loss: 4.87319565\n","Batch  100\n","val_loss: 7.72725439\n","Batch  200\n","val_loss: 7.54997492\n","Batch  300\n","val_loss: 9.89661026\n","Epoch: 63 | val_loss:8.00935132\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 64 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.00310189\n","Batch  100\n","train_loss: 0.00647978\n","Batch  200\n","train_loss: 0.01142714\n","Batch  300\n","train_loss: 0.01155898\n","Batch  400\n","train_loss: 0.00417643\n","Batch  500\n","train_loss: 0.05792477\n","Batch  600\n","train_loss: 0.00266933\n","Batch  700\n","train_loss: 0.00773553\n","Batch  800\n","train_loss: 0.01295662\n","Batch  900\n","train_loss: 0.00504774\n","Batch  1000\n","train_loss: 0.00534003\n","Batch  1100\n","train_loss: 0.00788879\n","Batch  1200\n","train_loss: 0.01206026\n","Batch  1300\n","train_loss: 0.00831832\n","Batch  1400\n","train_loss: 0.00738191\n","Batch  1500\n","train_loss: 0.01181957\n","Batch  1600\n","train_loss: 0.00627375\n","Batch  1700\n","train_loss: 0.01222365\n","Batch  1800\n","train_loss: 0.00865751\n","Batch  1900\n","train_loss: 0.01170204\n","Batch  2000\n","train_loss: 0.16543746\n","Batch  2100\n","train_loss: 0.01665040\n","Epoch: 64 | train_loss:0.01436375\n","Batch  0\n","val_loss: 4.75143862\n","Batch  100\n","val_loss: 7.88641310\n","Batch  200\n","val_loss: 7.22067404\n","Batch  300\n","val_loss: 9.51886845\n","Epoch: 64 | val_loss:8.12528270\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 65 | Time: 6m 21s ***********\n","Batch  0\n","train_loss: 0.07126951\n","Batch  100\n","train_loss: 0.00253846\n","Batch  200\n","train_loss: 0.00112298\n","Batch  300\n","train_loss: 0.00455104\n","Batch  400\n","train_loss: 0.00495594\n","Batch  500\n","train_loss: 0.00559262\n","Batch  600\n","train_loss: 0.00391861\n","Batch  700\n","train_loss: 0.00612470\n","Batch  800\n","train_loss: 0.00083646\n","Batch  900\n","train_loss: 0.00379469\n","Batch  1000\n","train_loss: 0.00781052\n","Batch  1100\n","train_loss: 0.00643264\n","Batch  1200\n","train_loss: 0.00728486\n","Batch  1300\n","train_loss: 0.00696825\n","Batch  1400\n","train_loss: 0.00818156\n","Batch  1500\n","train_loss: 0.01248486\n","Batch  1600\n","train_loss: 0.01291661\n","Batch  1700\n","train_loss: 0.00430251\n","Batch  1800\n","train_loss: 0.00982594\n","Batch  1900\n","train_loss: 0.00384533\n","Batch  2000\n","train_loss: 0.00512277\n","Batch  2100\n","train_loss: 0.00453259\n","Epoch: 65 | train_loss:0.01245558\n","Batch  0\n","val_loss: 4.89080715\n","Batch  100\n","val_loss: 7.80807447\n","Batch  200\n","val_loss: 7.32026482\n","Batch  300\n","val_loss: 9.84555721\n","Epoch: 65 | val_loss:8.22438186\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 66 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.00583420\n","Batch  100\n","train_loss: 0.00629702\n","Batch  200\n","train_loss: 0.00086304\n","Batch  300\n","train_loss: 0.00797521\n","Batch  400\n","train_loss: 0.00960832\n","Batch  500\n","train_loss: 0.00319286\n","Batch  600\n","train_loss: 0.00085827\n","Batch  700\n","train_loss: 0.00258019\n","Batch  800\n","train_loss: 0.00175211\n","Batch  900\n","train_loss: 0.00606506\n","Batch  1000\n","train_loss: 0.01135572\n","Batch  1100\n","train_loss: 0.00171741\n","Batch  1200\n","train_loss: 0.01479585\n","Batch  1300\n","train_loss: 0.00253879\n","Batch  1400\n","train_loss: 0.00279311\n","Batch  1500\n","train_loss: 0.00522562\n","Batch  1600\n","train_loss: 0.00351307\n","Batch  1700\n","train_loss: 0.00690395\n","Batch  1800\n","train_loss: 0.00743578\n","Batch  1900\n","train_loss: 0.00718498\n","Batch  2000\n","train_loss: 0.03550749\n","Batch  2100\n","train_loss: 0.05022905\n","Epoch: 66 | train_loss:0.01199517\n","Batch  0\n","val_loss: 4.72851610\n","Batch  100\n","val_loss: 8.15402794\n","Batch  200\n","val_loss: 7.87616444\n","Batch  300\n","val_loss: 9.37395096\n","Epoch: 66 | val_loss:8.37104397\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 67 | Time: 6m 21s ***********\n","Batch  0\n","train_loss: 0.00421887\n","Batch  100\n","train_loss: 0.00438012\n","Batch  200\n","train_loss: 0.00028834\n","Batch  300\n","train_loss: 0.00301688\n","Batch  400\n","train_loss: 0.00691749\n","Batch  500\n","train_loss: 0.00815597\n","Batch  600\n","train_loss: 0.00252386\n","Batch  700\n","train_loss: 0.00123938\n","Batch  800\n","train_loss: 0.00225782\n","Batch  900\n","train_loss: 0.00291339\n","Batch  1000\n","train_loss: 0.00125424\n","Batch  1100\n","train_loss: 0.00189092\n","Batch  1200\n","train_loss: 0.00322592\n","Batch  1300\n","train_loss: 0.04640608\n","Batch  1400\n","train_loss: 0.00495961\n","Batch  1500\n","train_loss: 0.00126278\n","Batch  1600\n","train_loss: 0.00827900\n","Batch  1700\n","train_loss: 0.00587101\n","Batch  1800\n","train_loss: 0.00035961\n","Batch  1900\n","train_loss: 0.00606879\n","Batch  2000\n","train_loss: 0.00658752\n","Batch  2100\n","train_loss: 0.00669824\n","Epoch: 67 | train_loss:0.01037980\n","Batch  0\n","val_loss: 5.05758142\n","Batch  100\n","val_loss: 8.00121307\n","Batch  200\n","val_loss: 7.90467548\n","Batch  300\n","val_loss: 10.06936169\n","Epoch: 67 | val_loss:8.40631663\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 68 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.00541430\n","Batch  100\n","train_loss: 0.00481027\n","Batch  200\n","train_loss: 0.00258370\n","Batch  300\n","train_loss: 0.00243205\n","Batch  400\n","train_loss: 0.00044351\n","Batch  500\n","train_loss: 0.00388702\n","Batch  600\n","train_loss: 0.00397196\n","Batch  700\n","train_loss: 0.00300242\n","Batch  800\n","train_loss: 0.00267732\n","Batch  900\n","train_loss: 0.05674252\n","Batch  1000\n","train_loss: 0.00299287\n","Batch  1100\n","train_loss: 0.00398109\n","Batch  1200\n","train_loss: 0.00526547\n","Batch  1300\n","train_loss: 0.00298232\n","Batch  1400\n","train_loss: 0.00523050\n","Batch  1500\n","train_loss: 0.00288296\n","Batch  1600\n","train_loss: 0.00467149\n","Batch  1700\n","train_loss: 0.00260951\n","Batch  1800\n","train_loss: 0.00998903\n","Batch  1900\n","train_loss: 0.01275064\n","Batch  2000\n","train_loss: 0.00249423\n","Batch  2100\n","train_loss: 0.00879127\n","Epoch: 68 | train_loss:0.00929736\n","Batch  0\n","val_loss: 4.93090677\n","Batch  100\n","val_loss: 8.07424736\n","Batch  200\n","val_loss: 8.02762222\n","Batch  300\n","val_loss: 10.04886913\n","Epoch: 68 | val_loss:8.51326007\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 69 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.00177749\n","Batch  100\n","train_loss: 0.00391653\n","Batch  200\n","train_loss: 0.00679733\n","Batch  300\n","train_loss: 0.00495134\n","Batch  400\n","train_loss: 0.00074479\n","Batch  500\n","train_loss: 0.00065868\n","Batch  600\n","train_loss: 0.00265147\n","Batch  700\n","train_loss: 0.00710614\n","Batch  800\n","train_loss: 0.00159620\n","Batch  900\n","train_loss: 0.00320664\n","Batch  1000\n","train_loss: 0.00427878\n","Batch  1100\n","train_loss: 0.00497692\n","Batch  1200\n","train_loss: 0.00286676\n","Batch  1300\n","train_loss: 0.00502282\n","Batch  1400\n","train_loss: 0.00424552\n","Batch  1500\n","train_loss: 0.00182769\n","Batch  1600\n","train_loss: 0.00211539\n","Batch  1700\n","train_loss: 0.00396232\n","Batch  1800\n","train_loss: 0.00267789\n","Batch  1900\n","train_loss: 0.00543395\n","Batch  2000\n","train_loss: 0.00139096\n","Batch  2100\n","train_loss: 0.00491202\n","Epoch: 69 | train_loss:0.00854272\n","Batch  0\n","val_loss: 5.11090851\n","Batch  100\n","val_loss: 8.22791195\n","Batch  200\n","val_loss: 7.54390049\n","Batch  300\n","val_loss: 9.93408298\n","Epoch: 69 | val_loss:8.54956772\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 70 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.00063601\n","Batch  100\n","train_loss: 0.00044579\n","Batch  200\n","train_loss: 0.00418496\n","Batch  300\n","train_loss: 0.00364466\n","Batch  400\n","train_loss: 0.00395344\n","Batch  500\n","train_loss: 0.00087162\n","Batch  600\n","train_loss: 0.00689843\n","Batch  700\n","train_loss: 0.00201402\n","Batch  800\n","train_loss: 0.00409231\n","Batch  900\n","train_loss: 0.00281371\n","Batch  1000\n","train_loss: 0.00274221\n","Batch  1100\n","train_loss: 0.00106123\n","Batch  1200\n","train_loss: 0.00378911\n","Batch  1300\n","train_loss: 0.00457686\n","Batch  1400\n","train_loss: 0.00293566\n","Batch  1500\n","train_loss: 0.00243508\n","Batch  1600\n","train_loss: 0.00076053\n","Batch  1700\n","train_loss: 0.00141057\n","Batch  1800\n","train_loss: 0.00574031\n","Batch  1900\n","train_loss: 0.00220057\n","Batch  2000\n","train_loss: 0.00413383\n","Batch  2100\n","train_loss: 0.00349358\n","Epoch: 70 | train_loss:0.00822552\n","Batch  0\n","val_loss: 4.84611940\n","Batch  100\n","val_loss: 8.74669266\n","Batch  200\n","val_loss: 7.36779451\n","Batch  300\n","val_loss: 10.38399792\n","Epoch: 70 | val_loss:8.64234527\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 71 | Time: 6m 26s ***********\n","Batch  0\n","train_loss: 0.00179514\n","Batch  100\n","train_loss: 0.00227320\n","Batch  200\n","train_loss: 0.00184978\n","Batch  300\n","train_loss: 0.00352049\n","Batch  400\n","train_loss: 0.00346183\n","Batch  500\n","train_loss: 0.00424682\n","Batch  600\n","train_loss: 0.00210518\n","Batch  700\n","train_loss: 0.00015390\n","Batch  800\n","train_loss: 0.00103486\n","Batch  900\n","train_loss: 0.00038605\n","Batch  1000\n","train_loss: 0.00300799\n","Batch  1100\n","train_loss: 0.00229789\n","Batch  1200\n","train_loss: 0.00227310\n","Batch  1300\n","train_loss: 0.00137426\n","Batch  1400\n","train_loss: 0.00298753\n","Batch  1500\n","train_loss: 0.00341974\n","Batch  1600\n","train_loss: 0.00050575\n","Batch  1700\n","train_loss: 0.00153943\n","Batch  1800\n","train_loss: 0.00232323\n","Batch  1900\n","train_loss: 0.00371920\n","Batch  2000\n","train_loss: 0.00405170\n","Batch  2100\n","train_loss: 0.00374335\n","Epoch: 71 | train_loss:0.00722816\n","Batch  0\n","val_loss: 4.84689093\n","Batch  100\n","val_loss: 8.36869240\n","Batch  200\n","val_loss: 8.02286243\n","Batch  300\n","val_loss: 10.48170376\n","Epoch: 71 | val_loss:8.67596150\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 72 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.00272681\n","Batch  100\n","train_loss: 0.00263000\n","Batch  200\n","train_loss: 0.00177041\n","Batch  300\n","train_loss: 0.00363455\n","Batch  400\n","train_loss: 0.00131962\n","Batch  500\n","train_loss: 0.00473222\n","Batch  600\n","train_loss: 0.00235491\n","Batch  700\n","train_loss: 0.00118465\n","Batch  800\n","train_loss: 0.00297251\n","Batch  900\n","train_loss: 0.00099416\n","Batch  1000\n","train_loss: 0.00299810\n","Batch  1100\n","train_loss: 0.00281159\n","Batch  1200\n","train_loss: 0.00326780\n","Batch  1300\n","train_loss: 0.00209854\n","Batch  1400\n","train_loss: 0.00471635\n","Batch  1500\n","train_loss: 0.00281197\n","Batch  1600\n","train_loss: 0.00154681\n","Batch  1700\n","train_loss: 0.00195416\n","Batch  1800\n","train_loss: 0.00147751\n","Batch  1900\n","train_loss: 0.03484994\n","Batch  2000\n","train_loss: 0.08295019\n","Batch  2100\n","train_loss: 0.00468162\n","Epoch: 72 | train_loss:0.00631783\n","Batch  0\n","val_loss: 4.72433329\n","Batch  100\n","val_loss: 8.79613876\n","Batch  200\n","val_loss: 8.27601337\n","Batch  300\n","val_loss: 10.74909496\n","Epoch: 72 | val_loss:8.73727238\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 73 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.00125728\n","Batch  100\n","train_loss: 0.00084009\n","Batch  200\n","train_loss: 0.00186695\n","Batch  300\n","train_loss: 0.00158877\n","Batch  400\n","train_loss: 0.00222272\n","Batch  500\n","train_loss: 0.00231380\n","Batch  600\n","train_loss: 0.00046735\n","Batch  700\n","train_loss: 0.00198080\n","Batch  800\n","train_loss: 0.17246710\n","Batch  900\n","train_loss: 0.00083280\n","Batch  1000\n","train_loss: 0.00331338\n","Batch  1100\n","train_loss: 0.00280934\n","Batch  1200\n","train_loss: 0.00363354\n","Batch  1300\n","train_loss: 0.00289981\n","Batch  1400\n","train_loss: 0.00092155\n","Batch  1500\n","train_loss: 0.00215914\n","Batch  1600\n","train_loss: 0.00271497\n","Batch  1700\n","train_loss: 0.01646479\n","Batch  1800\n","train_loss: 0.00031035\n","Batch  1900\n","train_loss: 0.00377955\n","Batch  2000\n","train_loss: 0.00034913\n","Batch  2100\n","train_loss: 0.00223396\n","Epoch: 73 | train_loss:0.00598629\n","Batch  0\n","val_loss: 4.97248936\n","Batch  100\n","val_loss: 8.99795246\n","Batch  200\n","val_loss: 8.46152496\n","Batch  300\n","val_loss: 10.65508366\n","Epoch: 73 | val_loss:8.79822481\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 74 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.00795148\n","Batch  100\n","train_loss: 0.00299274\n","Batch  200\n","train_loss: 0.00141594\n","Batch  300\n","train_loss: 0.00141538\n","Batch  400\n","train_loss: 0.00024034\n","Batch  500\n","train_loss: 0.00197177\n","Batch  600\n","train_loss: 0.00312001\n","Batch  700\n","train_loss: 0.00207414\n","Batch  800\n","train_loss: 0.00280192\n","Batch  900\n","train_loss: 0.00192688\n","Batch  1000\n","train_loss: 0.00136803\n","Batch  1100\n","train_loss: 0.00204871\n","Batch  1200\n","train_loss: 0.00247547\n","Batch  1300\n","train_loss: 0.00042323\n","Batch  1400\n","train_loss: 0.00194637\n","Batch  1500\n","train_loss: 0.05313639\n","Batch  1600\n","train_loss: 0.00055286\n","Batch  1700\n","train_loss: 0.00264554\n","Batch  1800\n","train_loss: 0.00337570\n","Batch  1900\n","train_loss: 0.00040618\n","Batch  2000\n","train_loss: 0.00099747\n","Batch  2100\n","train_loss: 0.00719976\n","Epoch: 74 | train_loss:0.00551974\n","Batch  0\n","val_loss: 5.03137445\n","Batch  100\n","val_loss: 8.84706974\n","Batch  200\n","val_loss: 8.42946720\n","Batch  300\n","val_loss: 10.62986755\n","Epoch: 74 | val_loss:8.81428289\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 75 | Time: 6m 22s ***********\n","Batch  0\n","train_loss: 0.00121893\n","Batch  100\n","train_loss: 0.00334770\n","Batch  200\n","train_loss: 0.00233517\n","Batch  300\n","train_loss: 0.00256626\n","Batch  400\n","train_loss: 0.00173565\n","Batch  500\n","train_loss: 0.00114110\n","Batch  600\n","train_loss: 0.00214934\n","Batch  700\n","train_loss: 0.00453115\n","Batch  800\n","train_loss: 0.00045532\n","Batch  900\n","train_loss: 0.00428799\n","Batch  1000\n","train_loss: 0.00226732\n","Batch  1100\n","train_loss: 0.00299309\n","Batch  1200\n","train_loss: 0.00210208\n","Batch  1300\n","train_loss: 0.00341131\n","Batch  1400\n","train_loss: 0.00129273\n","Batch  1500\n","train_loss: 0.00258675\n","Batch  1600\n","train_loss: 0.00124038\n","Batch  1700\n","train_loss: 0.00356626\n","Batch  1800\n","train_loss: 0.00058167\n","Batch  1900\n","train_loss: 0.00694664\n","Batch  2000\n","train_loss: 0.00061221\n","Batch  2100\n","train_loss: 0.00170534\n","Epoch: 75 | train_loss:0.00628249\n","Batch  0\n","val_loss: 5.31296444\n","Batch  100\n","val_loss: 9.00194550\n","Batch  200\n","val_loss: 8.65803146\n","Batch  300\n","val_loss: 11.06953430\n","Epoch: 75 | val_loss:8.85964548\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 76 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.00338510\n","Batch  100\n","train_loss: 0.00211921\n","Batch  200\n","train_loss: 0.00080346\n","Batch  300\n","train_loss: 0.00023466\n","Batch  400\n","train_loss: 0.00016113\n","Batch  500\n","train_loss: 0.00282385\n","Batch  600\n","train_loss: 0.00252098\n","Batch  700\n","train_loss: 0.00073096\n","Batch  800\n","train_loss: 0.00106052\n","Batch  900\n","train_loss: 0.00105060\n","Batch  1000\n","train_loss: 0.00469679\n","Batch  1100\n","train_loss: 0.00025623\n","Batch  1200\n","train_loss: 0.00230915\n","Batch  1300\n","train_loss: 0.13610134\n","Batch  1400\n","train_loss: 0.00151347\n","Batch  1500\n","train_loss: 0.00091307\n","Batch  1600\n","train_loss: 0.00224403\n","Batch  1700\n","train_loss: 0.00346338\n","Batch  1800\n","train_loss: 0.00165319\n","Batch  1900\n","train_loss: 0.00620983\n","Batch  2000\n","train_loss: 0.00151776\n","Batch  2100\n","train_loss: 0.00039338\n","Epoch: 76 | train_loss:0.00486129\n","Batch  0\n","val_loss: 5.25520182\n","Batch  100\n","val_loss: 8.48167801\n","Batch  200\n","val_loss: 8.62993336\n","Batch  300\n","val_loss: 10.93035507\n","Epoch: 76 | val_loss:8.92986087\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 77 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.00139123\n","Batch  100\n","train_loss: 0.00024966\n","Batch  200\n","train_loss: 0.00178795\n","Batch  300\n","train_loss: 0.00173006\n","Batch  400\n","train_loss: 0.00188602\n","Batch  500\n","train_loss: 0.00209096\n","Batch  600\n","train_loss: 0.00248545\n","Batch  700\n","train_loss: 0.00085548\n","Batch  800\n","train_loss: 0.00202788\n","Batch  900\n","train_loss: 0.00105334\n","Batch  1000\n","train_loss: 0.00223574\n","Batch  1100\n","train_loss: 0.00101140\n","Batch  1200\n","train_loss: 0.00168261\n","Batch  1300\n","train_loss: 0.00113776\n","Batch  1400\n","train_loss: 0.00040163\n","Batch  1500\n","train_loss: 0.00245897\n","Batch  1600\n","train_loss: 0.00044983\n","Batch  1700\n","train_loss: 0.00162713\n","Batch  1800\n","train_loss: 0.00296810\n","Batch  1900\n","train_loss: 0.00025140\n","Batch  2000\n","train_loss: 0.00341406\n","Batch  2100\n","train_loss: 0.00168968\n","Epoch: 77 | train_loss:0.00475401\n","Batch  0\n","val_loss: 5.30387783\n","Batch  100\n","val_loss: 8.68868542\n","Batch  200\n","val_loss: 8.85691261\n","Batch  300\n","val_loss: 11.38920116\n","Epoch: 77 | val_loss:8.99981080\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 78 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.00039749\n","Batch  100\n","train_loss: 0.00170277\n","Batch  200\n","train_loss: 0.00095526\n","Batch  300\n","train_loss: 0.00087514\n","Batch  400\n","train_loss: 0.00120678\n","Batch  500\n","train_loss: 0.00285594\n","Batch  600\n","train_loss: 0.00219189\n","Batch  700\n","train_loss: 0.00043232\n","Batch  800\n","train_loss: 0.00101025\n","Batch  900\n","train_loss: 0.00155350\n","Batch  1000\n","train_loss: 0.00193494\n","Batch  1100\n","train_loss: 0.00205491\n","Batch  1200\n","train_loss: 0.00276915\n","Batch  1300\n","train_loss: 0.00163938\n","Batch  1400\n","train_loss: 0.00295880\n","Batch  1500\n","train_loss: 0.00277013\n","Batch  1600\n","train_loss: 0.00280295\n","Batch  1700\n","train_loss: 0.01942739\n","Batch  1800\n","train_loss: 0.00237484\n","Batch  1900\n","train_loss: 0.00166250\n","Batch  2000\n","train_loss: 0.00275519\n","Batch  2100\n","train_loss: 0.00247717\n","Epoch: 78 | train_loss:0.00537011\n","Batch  0\n","val_loss: 4.98903322\n","Batch  100\n","val_loss: 8.84799767\n","Batch  200\n","val_loss: 8.42603016\n","Batch  300\n","val_loss: 11.06323433\n","Epoch: 78 | val_loss:9.03930723\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 79 | Time: 6m 23s ***********\n","Batch  0\n","train_loss: 0.00029447\n","Batch  100\n","train_loss: 0.00037954\n","Batch  200\n","train_loss: 0.00171131\n","Batch  300\n","train_loss: 0.00022808\n","Batch  400\n","train_loss: 0.00042538\n","Batch  500\n","train_loss: 0.00042447\n","Batch  600\n","train_loss: 0.00147919\n","Batch  700\n","train_loss: 0.00150483\n","Batch  800\n","train_loss: 0.00286592\n","Batch  900\n","train_loss: 0.00030496\n","Batch  1000\n","train_loss: 0.00237022\n","Batch  1100\n","train_loss: 0.00115377\n","Batch  1200\n","train_loss: 0.00145777\n","Batch  1300\n","train_loss: 0.00350408\n","Batch  1400\n","train_loss: 0.00229949\n","Batch  1500\n","train_loss: 0.00058305\n","Batch  1600\n","train_loss: 0.00043709\n","Batch  1700\n","train_loss: 0.00070320\n","Batch  1800\n","train_loss: 0.00119622\n","Batch  1900\n","train_loss: 0.08371548\n","Batch  2000\n","train_loss: 0.00111106\n","Batch  2100\n","train_loss: 0.00117168\n","Epoch: 79 | train_loss:0.00494531\n","Batch  0\n","val_loss: 5.14334679\n","Batch  100\n","val_loss: 8.59655762\n","Batch  200\n","val_loss: 8.42286587\n","Batch  300\n","val_loss: 11.23721123\n","Epoch: 79 | val_loss:9.09918869\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 80 | Time: 6m 24s ***********\n","Batch  0\n","train_loss: 0.00029296\n","Batch  100\n","train_loss: 0.00119446\n","Batch  200\n","train_loss: 0.00180633\n","Batch  300\n","train_loss: 0.00113880\n","Batch  400\n","train_loss: 0.00032995\n","Batch  500\n","train_loss: 0.00149870\n","Batch  600\n","train_loss: 0.00117356\n","Batch  700\n","train_loss: 0.00166020\n","Batch  800\n","train_loss: 0.00048816\n","Batch  900\n","train_loss: 0.00109600\n","Batch  1000\n","train_loss: 0.00194159\n","Batch  1100\n","train_loss: 0.00161891\n","Batch  1200\n","train_loss: 0.00206033\n","Batch  1300\n","train_loss: 0.09369203\n","Batch  1400\n","train_loss: 0.00199454\n","Batch  1500\n","train_loss: 0.00196174\n","Batch  1600\n","train_loss: 0.00094401\n","Batch  1700\n","train_loss: 0.00120387\n","Batch  1800\n","train_loss: 0.00017526\n","Batch  1900\n","train_loss: 0.00017930\n","Batch  2000\n","train_loss: 0.00238409\n","Batch  2100\n","train_loss: 0.00055387\n","Epoch: 80 | train_loss:0.00509412\n","Batch  0\n","val_loss: 5.37165022\n","Batch  100\n","val_loss: 9.09549236\n","Batch  200\n","val_loss: 8.40884781\n","Batch  300\n","val_loss: 11.48622322\n","Epoch: 80 | val_loss:9.13775595\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 81 | Time: 6m 25s ***********\n","Batch  0\n","train_loss: 0.00075570\n","Batch  100\n","train_loss: 0.00154020\n","Batch  200\n","train_loss: 0.00161840\n","Batch  300\n","train_loss: 0.01636308\n","Batch  400\n","train_loss: 0.00022087\n","Batch  500\n","train_loss: 0.00124864\n","Batch  600\n","train_loss: 0.00014708\n","Batch  700\n","train_loss: 0.00253090\n","Batch  800\n","train_loss: 0.00219281\n","Batch  900\n","train_loss: 0.00016392\n","Batch  1000\n","train_loss: 0.00119740\n","Batch  1100\n","train_loss: 0.00086838\n","Batch  1200\n","train_loss: 0.00039435\n","Batch  1300\n","train_loss: 0.00195559\n","Batch  1400\n","train_loss: 0.00146472\n","Batch  1500\n","train_loss: 0.00121776\n","Batch  1600\n","train_loss: 0.00202163\n","Batch  1700\n","train_loss: 0.00149325\n","Batch  1800\n","train_loss: 0.00320848\n","Batch  1900\n","train_loss: 0.00070964\n","Batch  2000\n","train_loss: 0.00163501\n","Batch  2100\n","train_loss: 0.00289875\n","Epoch: 81 | train_loss:0.00456062\n","Batch  0\n","val_loss: 5.60984802\n","Batch  100\n","val_loss: 8.78232861\n","Batch  200\n","val_loss: 7.75626755\n","Batch  300\n","val_loss: 11.20648193\n","Epoch: 81 | val_loss:9.11134173\n","scheduler adjust learning rate to  0.0001\n","*************Epoch: 82 | Time: 6m 19s ***********\n","Batch  0\n","train_loss: 0.00181523\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-dadb1c489cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0meval_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-74ae649253e0>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, data_iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"_eVbNSm83J9Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607478209143,"user_tz":300,"elapsed":1511,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"f730ec79-e97a-4889-afba-8b118081eeb3"},"source":["enc_hid_dim = 512\n","dec_hid_dim = 512\n","attn_dim = 64\n","PAD_idx = TEXT.vocab.stoi['<pad>']\n","attn = Attention(enc_hid_dim, dec_hid_dim, attn_dim)\n","enc = EncoderBiRNN(pretrained_embed=TEXT.vocab.vectors, padding_idx=TEXT.vocab.stoi[TEXT.pad_token], enc_hid_dim=enc_hid_dim, dec_hid_dim=dec_hid_dim)\n","dec = Decoder(pretrained_embed=TEXT.vocab.vectors, padding_idx=TEXT.vocab.stoi[TEXT.pad_token], enc_hid_dim=enc_hid_dim, dec_hid_dim=dec_hid_dim, attention=attn)\n","model = Seq2Seq(enc, dec, PAD_idx, device).to(device)\n","\n","model.load_state_dict(torch.load('/content/drive/MyDrive/gru/model_9.13775595_80.pt'))\n","model.eval()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): EncoderBiRNN(\n","    (embedding): Embedding(33069, 300, padding_idx=1)\n","    (gru): GRU(300, 512, batch_first=True, bidirectional=True)\n","    (fc): Linear(in_features=1024, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=1536, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (embedding): Embedding(33069, 300, padding_idx=1)\n","    (gru): GRU(1324, 512, batch_first=True)\n","    (out): Linear(in_features=1836, out_features=33069, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3L2SI9eqPcGT","executionInfo":{"status":"ok","timestamp":1607382545672,"user_tz":300,"elapsed":19947,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"d7eacc48-a204-4d44-b557-8c3fefb64492"},"source":["learning_rate = 1e-3\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate,)\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, min_lr=1e-9)\n","criterion = nn.CrossEntropyLoss(ignore_index = TEXT.vocab.stoi['<pad>'])\n","\n","eval_epoch(0, model, test_iterator, scheduler, criterion)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Batch  0\n","val_loss: 2.54023647\n","Batch  100\n","val_loss: 3.70880771\n","Batch  200\n","val_loss: 3.79370332\n","Batch  300\n","val_loss: 5.61560869\n","Epoch: 0 | val_loss:4.34996969\n","scheduler adjust learning rate to  0.001\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["4.349969694129375"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"zNEAa9ExPr7Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607478234903,"user_tz":300,"elapsed":1296,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"c78f35bd-e43e-4fbc-d1a6-558e30708fb1"},"source":["def translate_sentence(sentence, src_field, model, device, max_len = 100):\n","  model.eval()\n","  nlp = spacy.load('en')\n","  tokens = [src_field.init_token] + [token.text.lower() for token in nlp(sentence)] + [src_field.eos_token]\n","\n","  src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","\n","  src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n","  src_len = torch.LongTensor([len(src_indexes)]).to(device)\n","\n","  with torch.no_grad():\n","    initHidden = torch.zeros((2, 1, enc_hid_dim)).to(device)\n","    encoder_outputs, hidden = model.encoder(src_tensor, src_len, initHidden)\n","\n","  mask = model.create_mask(src_tensor)\n","  \n","  trg_indexes = [src_field.vocab.stoi[src_field.init_token]]\n","  attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n","\n","  for i in range(max_len):\n","    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","    with torch.no_grad():\n","      output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n","    \n","    attentions[i] = attention\n","    pred_token = output.argmax(1).item()\n","    trg_indexes.append(pred_token)\n","    if pred_token == src_field.vocab.stoi[src_field.eos_token]:\n","            break\n","\n","  trg_tokens = [src_field.vocab.itos[i] for i in trg_indexes]\n","\n","  return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n","\n","\n","translate_sentence(\"how are you Nicole\", TEXT, model,device)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['how', 'are', 'you', 'nicole', '<eos>'],\n"," tensor([[[8.7455e-02, 6.7911e-01, 8.9767e-02, 3.5069e-02, 3.5552e-02,\n","           7.3048e-02]],\n"," \n","         [[2.3584e-02, 1.0861e-01, 7.9780e-01, 6.0897e-02, 8.3767e-03,\n","           7.3422e-04]],\n"," \n","         [[5.2186e-02, 1.8997e-02, 5.2403e-02, 6.4394e-01, 2.2600e-01,\n","           6.4728e-03]],\n"," \n","         [[7.7604e-03, 3.0207e-03, 2.0119e-03, 3.6106e-02, 8.7945e-01,\n","           7.1655e-02]],\n"," \n","         [[1.2719e-02, 7.2070e-03, 1.3895e-03, 1.2865e-02, 2.4963e-01,\n","           7.1619e-01]]], device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"9QZXaEuvZ-bh","executionInfo":{"status":"ok","timestamp":1607478394783,"user_tz":300,"elapsed":537,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}}},"source":["import json\n","import csv\n","\n","with open('/content/treccastweb/2019/data/evaluation/evaluation_topics_v1.0.json', 'r') as f:\n","  data = json.load(f)\n","\n","with open('eval_query.tsv', 'w') as f:\n","  tsv_writer = csv.writer(f, delimiter='\\t')\n","\n","  for item in data:\n","    qid = item['number']\n","    prev = []\n","    for turn in item['turn']:\n","      id = str(qid) + '_' + str(turn['number'])\n","      tsv_writer.writerow([id, ' ||| '.join(prev + [turn['raw_utterance']])])\n","      prev.append(turn['raw_utterance'])\n","   "],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BBVggSoMbNg0","executionInfo":{"status":"error","timestamp":1607478509045,"user_tz":300,"elapsed":96754,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"d94e31be-1a87-4831-9af1-ac709a0aecd3"},"source":["import csv\n","\n","with open('query_rewritten_gru.tsv', 'w') as outfile:\n","  tsv_writer = csv.writer(outfile, delimiter='\\t')\n","\n","  with open('eval_query.tsv', 'r') as f:\n","    read_tsv = csv.reader(f, delimiter=\"\\t\")\n","    for line in read_tsv:\n","      qid, query = line\n","      rewritten, _ = translate_sentence(query, TEXT, model, device)\n","      rewritten = ' '.join(rewritten[:-1])\n","      print(rewritten)\n","      # tsv_writer.writerow([qid, rewritten])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["what is amitabh cancer ?\n","is is cancer cancer in the 1976 ?\n","tell me cancer cancer cancer .\n","what are the lung cancer ?\n","can the <unk> can can the the hattie mcdaniel ?\n","what causes lung cancer in relation to cancer ?\n","what is the first sign of the cancer cancer ?\n","is the first of the cancer as same cancer , cancer ?\n","what 's the in in the cancer bats and cancer cancer ?\n","what are the different types of of of the fantastic of of the ?\n","if the song , is the eagles \" is ?\n","tell me more about the death of the article ?\n","what is the largest ever to have living on earth ?\n","what 's the biggest ever was in the article on the ?\n","what does for great the sweet , the fight ?\n","tell does for the cult , are there about\n","what are the show of <unk> about ?\n","where do the naga people ?\n","what do the band members on the cult ?\n","how do the price of the live on the philadelphia phillies being promoted for ?\n","tell me about the film story film .\n","what is the film story about ?\n","how is the film story about ?\n","did the film story film win any awards ?\n","is it the film story film by the a story first ?\n","who was the author and when who the it story published ?\n","what are the main theme in the film story of \" how ?\n","who are the main characters in the film story ?\n","what are the differences between the book \" the film story and movies ?\n","did the horse 's really of die ?\n","tell me about the bronze age . ?\n","what is the evidence for the \" age of ?\n","what are some of the two causes of the the age of <unk> ?\n","who were the sea peoples that the the age <unk> ?\n","what was the 's peoples in    ?\n","what other other led to to a of <unk> other than the <unk> of <unk> ?\n","what are the factors of the bobet of <unk> ?\n","what native american did the <unk> <unk> ?\n","what came after the death of the <unk> <unk> ?\n","tell me about the history of manorialism .\n","where does the term come from ?\n","where and when was the first of <unk> from the first invention ?\n","why do the    call as a of the ?\n","what came before the the call of the ?\n","when did the other of \" by the other than become ?\n","how many types are there in the history of . ?\n","why are the two so important for the sweet of the simpsons ?\n","what is the role of film in ?\n","what is the us political college ?\n","how does the us is ?\n","what is the us is college about ?\n","why was the system of the <unk> <unk> ' ?\n","how was the system of <unk> changed over time ?\n","what if the republican do n't <unk> for the senate candidate ?\n","how has the <unk> committee changed in 2012 ?\n","who are the the facts members to the talking heads to to do ?\n","how would the the the future be be ?\n","how does the national popular <unk> to the <unk> ?\n","is the the most new legal ?\n","what was the university experiment ?\n","what did the the experiment show ?\n","tell me the the of the author of the experiment ?\n","was the the experiment ?\n","what are other similar <unk> for the experiment other than the university ?\n","what happened in the to experiment in the experiment ?\n","why was the the why was the in the ?\n","what were the similarities and different between the studies of the experiment ?\n","what about the the experiment <unk> the university experiment ?\n","what are the key of the the experiment ?\n","how did the results of the the <unk> <unk> from the university ?\n","why was the the lawsuit involving with the ?\n","what is the disease in ?\n","how do you get an disease ?\n","how does the get in the get ?\n","what happens else the the birth of    ?\n","can the two children get your kill ?\n","how is is the test in the get of the ?\n","can the is the two live is ?\n","what is the <unk> in the ?\n","what does the budget mean to be a ? ?\n","what is the support argument for the \" why ?\n","do you have the for for the slayer ?\n","other than the good , for are there any other good of of other than the <unk> ?\n","if you you do n't any hot , at all , is there for you ?\n","what are good sources of health <unk> ?\n","does \" good vibrations by you light ?\n","what about the health effects health and health for ?\n","how does the impact impact compare between the article ?\n","what are the origins of popular music of the music ?\n","what are the the of of <unk> the <unk> of ?\n","what happened of the <unk> of the music of the music ?\n","when and why did the start <unk> from the music industry ?\n","how has the indigenous been has into music education ?\n","describe some of the cult pop bands .\n","what makes a song from rock musical ?\n","what is the difference between the and the cult ?\n","how did british change music ?\n","what are the roots and what influences and what influenced the ?\n","why is the drinking in in the us in and 15 years ?\n","what were the indigenous and in the & between and us ?\n","tell me about the negative health effects of the . health of red . ?\n","how does the alcohol affect affect development of the us ?\n","how does the factors of the health affect affect health health ?\n","what are the short and long - term of the on the moon on the 2012 ?\n","are there typically sober in the article ?\n","what are the effects on the undertones on the ?\n","are there any are in the the the mental health problems ?\n","what is the famous for for ?\n","what is there to do in the turkana ?\n","are there other activities activities related to the turkana or train ?\n","are there any related to patti smith ?\n","can there any other to be like other than to charles miller ?\n","what is rock city , and and is famous ?\n","are the special events held in the and the famous ?\n","what kind of food is is known known ?\n","what are some interesting things around ann 's in regards to ?\n","tell me tell about the the city was founded .\n","how does the time of the the of the university ?\n","does the the of the museum of art have any special collection ?\n","what is the south 's tour at ?\n","what are the film festivals of the university ?\n","how does the film of the for the ?\n","what are popular a or the where where i can let to the music ?\n","what causes 2016 , in the morning ?\n","does the have have term term side effects ?\n","what is the best for for the daily ?\n","what are the side effects of long term of the use ?\n","tell me about the treatments of the vlachs use ?\n","what food cause the two of the days ?\n","what the <unk> disease take the new album ?\n","how does the affect on the album \" louie ? \"\n","what is is is the best for playing ?\n","what kind should i get if i is ?\n","how does big and active in ?\n","do big members of the group live longer than small animals ?\n","will does want to put a lot of time into training in <unk> ?\n","tell me about some famous that are a positive and can be left home all day ?\n","how much does a \" song on the article , and how is a <unk> ?\n","how much do the \" , how much for the ?\n","why is the a second language difficult ?\n","at what age was second \" really is \" way \" why ?\n","if did the song i should know , \" to the ?\n","how can i begin learning as ?\n","is the different is to better than english ?\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d601c25e0a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_tsv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mrewritten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mrewritten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewritten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewritten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-3214f239f282>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(sentence, src_field, model, device, max_len)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_token\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# in data dir / shortcut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_link\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE051\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/data/en/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, disable)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta.json\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         deserializers[\"vocab\"] = lambda p: self.vocab.from_disk(\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         ) and _fix_pretrained_vectors_name(self)\n\u001b[1;32m    933\u001b[0m         deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(\n","\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lookups.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lookups.py\u001b[0m in \u001b[0;36mfrom_bytes\u001b[0;34m(self, bytes_data, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsgpack_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lookups.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, data)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbloom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBloomFilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTdPm_W8g9k-","executionInfo":{"status":"ok","timestamp":1607478372769,"user_tz":300,"elapsed":3083,"user":{"displayName":"Nicole Yan","photoUrl":"","userId":"03590027735766332427"}},"outputId":"c46f84cc-c4f7-4ed3-e09f-cfee830d0cc6"},"source":["!git clone https://github.com/daltonj/treccastweb.git"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Cloning into 'treccastweb'...\n","remote: Enumerating objects: 5, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 430 (delta 0), reused 0 (delta 0), pack-reused 425\u001b[K\n","Receiving objects: 100% (430/430), 13.04 MiB | 11.95 MiB/s, done.\n","Resolving deltas: 100% (186/186), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HhuK0BsWLV1a"},"source":[""],"execution_count":null,"outputs":[]}]}