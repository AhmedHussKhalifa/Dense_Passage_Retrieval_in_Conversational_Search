{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedHussKhalifa/DPR/blob/master/DPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PswzWXeSRnYN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "675XhvjWqQ5O",
        "outputId": "4cf4bcd2-283e-4e0f-ace4-31b08b36f576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ! git clone https://github.com/facebookresearch/DPR\n",
        "! git clone https://github.com/AhmedHussKhalifa/DPR\n",
        "! python --version # now returns Python 3.6.5 :: Anaconda, Inc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DPR' already exists and is not an empty directory.\n",
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lp9208hHxcK",
        "outputId": "014665e2-b95c-4f7c-944d-7061f8ddea4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "cd DPR\n",
        "pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/DPR\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (0.29.21)\n",
            "Requirement already satisfied: faiss-cpu>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (1.6.4.post2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: transformers<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (4.41.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (3.2)\n",
            "Requirement already satisfied: spacy>=2.1.8 in /usr/local/lib/python3.6/dist-packages (from dpr==0.1.0) (2.2.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->dpr==0.1.0) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->dpr==0.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->dpr==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.8.1rc1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.1.0,>=3.0.0->dpr==0.1.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.8->dpr==0.1.0) (2.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (0.17.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.1.0,>=3.0.0->dpr==0.1.0) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.8->dpr==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.8->dpr==0.1.0) (3.4.0)\n",
            "Building wheels for collected packages: dpr\n",
            "  Building wheel for dpr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dpr: filename=dpr-0.1.0-cp36-none-any.whl size=12891 sha256=c49b7c0940e0b9cf7de28f3f1d460f514b6bdf5c33a718e4b57a4e0c19e65829\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g3v329j0/wheels/7b/1b/8b/cbf33220fb7d3eb120e18f746a5b752ebc53f8523fb2eb0681\n",
            "Successfully built dpr\n",
            "Installing collected packages: dpr\n",
            "  Found existing installation: dpr 0.1.0\n",
            "    Uninstalling dpr-0.1.0:\n",
            "      Successfully uninstalled dpr-0.1.0\n",
            "Successfully installed dpr-0.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDW2_573IWZ2",
        "outputId": "769544d9-79ec-42c5-eb40-e909877a412a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download NQ datasets\n",
        "%%shell\n",
        "cd DPR\n",
        "python data/download_data.py --resource data.retriever.qas.nq\n",
        "\n",
        "python data/download_data.py --resource data.retriever.nq\n",
        "\n",
        "# mkdir data/retriever_1\n",
        "cd data/retriever\n",
        "wget https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz -O nq-dev.json.gz\n",
        "gzip -d nq-dev.json.gz\n",
        "ls -sh nq-dev.json\n",
        "\n",
        "# commands below download the full wikipedia dataset and nq-train datasets, which are\n",
        "# too huge to load. I leave them here in case anyone wants to try the full datasets.\n",
        "# We can use nq-train-subset.json and psgs_w100_subset.tsv to test DPR.\n",
        "\n",
        "# cd data/wikipedia_split\n",
        "# wget https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz \n",
        "# gzip -d psgs_w100.tsv.gz\n",
        "\n",
        "# cd ../retriever\n",
        "# wget https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz -O nq-train.json.gz\n",
        "# gzip -d nq-train.json.gz\n",
        "# ls -sh nq-train.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from  https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-dev.qa.csv\n",
            "File already exist  ./data/retriever/qas/nq-dev.csv\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
            "File already exist  ./data/retriever/qas/LICENSE\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
            "File already exist  ./data/retriever/qas/README\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.qa.csv\n",
            "File already exist  ./data/retriever/qas/nq-test.csv\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
            "File already exist  ./data/retriever/qas/LICENSE\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
            "File already exist  ./data/retriever/qas/README\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-train.qa.csv\n",
            "File already exist  ./data/retriever/qas/nq-train.csv\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
            "File already exist  ./data/retriever/qas/LICENSE\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
            "File already exist  ./data/retriever/qas/README\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\n",
            "Saved to  ./data/retriever/nq-dev.tmp\n",
            "Uncompressing  ./data/retriever/nq-dev.tmp\n",
            "Saved to  ./data/retriever/nq-dev.json\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
            "File already exist  ./data/retriever/LICENSE\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
            "File already exist  ./data/retriever/README\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\n",
            "File already exist  ./data/retriever/nq-train.tmp\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
            "File already exist  ./data/retriever/LICENSE\n",
            "Loading from  https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
            "File already exist  ./data/retriever/README\n",
            "--2020-11-09 18:55:09--  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 256239282 (244M) [application/gzip]\n",
            "Saving to: ‘nq-dev.json.gz’\n",
            "\n",
            "nq-dev.json.gz      100%[===================>] 244.37M  12.4MB/s    in 21s     \n",
            "\n",
            "2020-11-09 18:55:31 (11.6 MB/s) - ‘nq-dev.json.gz’ saved [256239282/256239282]\n",
            "\n",
            "gzip: nq-dev.json already exists; do you wish to overwrite (y or n)? "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfTwhkE-4E_S"
      },
      "source": [
        "%%shell\n",
        "# THis for Downloading all the resourses\n",
        "# Do not run that \n",
        "cd DPR\n",
        "# python data/download_data.py --resource data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvPvdSVRnQfc"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd DPR\n",
        "\n",
        "python data/download_data.py --resource data.retriever.nq-train \n",
        "\n",
        "python data/download_data.py --resource data.retriever.nq-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcZFFjJQt-JS"
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd DPR\n",
        "\n",
        "python -m torch.distributed.launch \\\n",
        "\t--nproc_per_node=1 train_dense_encoder.py \\\n",
        "\t--max_grad_norm 2.0 \\\n",
        "\t--encoder_model_type hf_bert \\\n",
        "\t--pretrained_model_cfg bert-base-uncased \\\n",
        "\t--seed 12345 \\\n",
        "\t--sequence_length 256 \\\n",
        "\t--warmup_steps 1237 \\\n",
        "\t--batch_size 2 \\\n",
        "\t--do_lower_case \\\n",
        "\t--train_file \"/content/DPR/data/retriever/nq-train.tmp\" \\\n",
        "\t--dev_file \"/content/DPR/data/retriever/nq-dev.json\" \\\n",
        "\t--output_dir \"/content/DPR/output\" \\\n",
        "\t--learning_rate 2e-05 \\\n",
        "\t--num_train_epochs 40 \\\n",
        "\t--dev_batch_size 16 \\\n",
        "  --val_av_rank_start_epoch 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODkIWTrbv57e",
        "outputId": "307e5b7d-a1db-4104-9ce1-3cc371bc6b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd DPR\n",
        "\n",
        "# python data/download_data.py --resource data.retriever_results.nq.single.wikipedia_passages\n",
        "\n",
        "python generate_dense_embeddings.py \\\n",
        "\t--model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n",
        "\t--ctx_file \"/content/DPR/data/retriever_results/nq/single/wikipedia_passages_0.pkl\" \\\n",
        "\t--shard_id 0 \\\n",
        "  --num_shards 1 \\\n",
        "\t--out_file \"inference\"\n",
        "\n",
        "# python generate_dense_embeddings.py \\\n",
        "#   --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n",
        "#   --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\n",
        "#   --shard_id 0 \\\n",
        "#   --num_shards 1 \\\n",
        "#   --out_file inference"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no resources found for specified key\n",
            "Initialized host 7f048dd76ae9 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n",
            "Reading saved model from /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_dense_embeddings.py\", line 144, in <module>\n",
            "    main(args)\n",
            "  File \"generate_dense_embeddings.py\", line 74, in main\n",
            "    saved_state = load_states_from_checkpoint(args.model_file)\n",
            "  File \"/content/DPR/dpr/utils/model_utils.py\", line 140, in load_states_from_checkpoint\n",
            "    state_dict = torch.load(model_file, map_location=lambda s, l: default_restore_location(s, 'cpu'))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 581, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e12e1f315dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ncd DPR\\n\\n# python data/download_data.py --resource data.retriever_results.nq.single.wikipedia_passages\\npython data/download_data.py --resource dpr_biencoder.0.919\\npython generate_dense_embeddings.py \\\\\\n\\t--model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\\\\n\\t--ctx_file \"/content/DPR/data/retriever_results/nq/single/wikipedia_passages_0.pkl\" \\\\\\n\\t--shard_id 0 \\\\\\n  --num_shards 1 \\\\\\n\\t--out_file \"inference\"\\n\\n# python generate_dense_embeddings.py \\\\\\n#   --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\\\\n#   --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\\\\n#   --shard_id 0 \\\\\\n#   --num_shards 1 \\\\\\n#   --out_file inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 138\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '\ncd DPR\n\n# python data/download_data.py --resource data.retriever_results.nq.single.wikipedia_passages\npython data/download_data.py --resource dpr_biencoder.0.919\npython generate_dense_embeddings.py \\\n\t--model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n\t--ctx_file \"/content/DPR/data/retriever_results/nq/single/wikipedia_passages_0.pkl\" \\\n\t--shard_id 0 \\\n  --num_shards 1 \\\n\t--out_file \"inference\"\n\n# python generate_dense_embeddings.py \\\n#   --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n#   --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\n#   --shard_id 0 \\\n#   --num_shards 1 \\\n#   --out_file inference' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97kXIct7EF5x",
        "outputId": "84e8b244-afb7-4c8a-f26c-1af817b46d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "# mkdir collections/msmarco-doc\n",
        "\n",
        "# wget https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.trec.gz -P collections/msmarco-doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘collections/msmarco-doc’: No such file or directory\n",
            "--2020-11-09 08:12:42--  https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-docs.trec.gz\n",
            "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 40.112.152.16\n",
            "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|40.112.152.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8501799926 (7.9G) [application/x-gzip]\n",
            "Saving to: ‘collections/msmarco-doc/msmarco-docs.trec.gz’\n",
            "\n",
            "msmarco-docs.trec.g 100%[===================>]   7.92G  23.0MB/s    in 6m 18s  \n",
            "\n",
            "2020-11-09 08:19:01 (21.5 MB/s) - ‘collections/msmarco-doc/msmarco-docs.trec.gz’ saved [8501799926/8501799926]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Vfhy53C02a",
        "outputId": "92c2352b-1179-4f39-e9bc-0639918a7278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Training on nq-train-subset.json\n",
        "# Dont forget to change the paths based on your directory structure\n",
        "%%shell\n",
        "cd DPR\n",
        "# ls -sh \"/content/drive/My Drive/nq-train-subset.json\"\n",
        "ls -sh \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\n",
        "ls \"/content/drive/My Drive/STAT946/DPR/output/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220M '/content/drive/My Drive/STAT946/DPR/nq-train-subset.json'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5jRpNOUamsL",
        "outputId": "c977d434-b998-41e5-aecf-5a7bca40e96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%shell\n",
        "cd DPR\n",
        "ls -sh \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\n",
        "ls -sh \"data/retriever/nq-dev.json\" \n",
        "[ -d \"/content/drive/My Drive/STAT946/DPR/output/\" ] && \\\n",
        "echo \"Directory /content/drive/My Drive/STAT946/DPR/output/ exists.\"\n",
        "\n",
        "python train_dense_encoder.py \\\n",
        "  --encoder_model_type hf_bert \\\n",
        "  --pretrained_model_cfg bert-base-uncased \\\n",
        "  --train_file \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\\\n",
        "  --dev_file \"data/retriever/nq-dev.json\" \\\n",
        "  --output_dir \"/content/drive/My Drive/STAT946/DPR/output\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220M '/content/drive/My Drive/STAT946/DPR/nq-train-subset.json'\n",
            "778M data/retriever/nq-dev.json\n",
            "Directory /content/drive/My Drive/STAT946/DPR/output/ exists.\n",
            "Initialized host ab29005f0866 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n",
            " **************** CONFIGURATION **************** \n",
            "adam_betas                     -->   (0.9, 0.999)\n",
            "adam_eps                       -->   1e-08\n",
            "batch_size                     -->   2\n",
            "checkpoint_file_name           -->   dpr_biencoder\n",
            "dev_batch_size                 -->   4\n",
            "dev_file                       -->   data/retriever/nq-dev.json\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   False\n",
            "dropout                        -->   0.1\n",
            "encoder_model_type             -->   hf_bert\n",
            "eval_per_epoch                 -->   1\n",
            "fix_ctx_encoder                -->   False\n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "global_loss_buf_sz             -->   150000\n",
            "gradient_accumulation_steps    -->   1\n",
            "hard_negatives                 -->   1\n",
            "learning_rate                  -->   1e-05\n",
            "local_rank                     -->   -1\n",
            "log_batch_step                 -->   100\n",
            "max_grad_norm                  -->   1.0\n",
            "model_file                     -->   None\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "num_train_epochs               -->   3.0\n",
            "other_negatives                -->   0\n",
            "output_dir                     -->   /content/drive/My Drive/STAT946/DPR/output\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   bert-base-uncased\n",
            "projection_dim                 -->   0\n",
            "seed                           -->   0\n",
            "sequence_length                -->   512\n",
            "shuffle_positive_ctx           -->   False\n",
            "train_file                     -->   /content/drive/My Drive/STAT946/DPR/nq-train-subset.json\n",
            "train_files_upsample_rates     -->   None\n",
            "train_rolling_loss_step        -->   100\n",
            "val_av_rank_bsz                -->   128\n",
            "val_av_rank_hard_neg           -->   30\n",
            "val_av_rank_max_qs             -->   10000\n",
            "val_av_rank_other_neg          -->   30\n",
            "val_av_rank_start_epoch        -->   10000\n",
            "warmup_steps                   -->   100\n",
            "weight_decay                   -->   0.0\n",
            " **************** CONFIGURATION **************** \n",
            "***** Initializing components for training *****\n",
            "Checkpoint files []\n",
            "PyTorch version 1.7.0+cu101 available.\n",
            "2020-11-04 18:15:38.248772: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "TensorFlow version 2.3.0 available.\n",
            "Lock 140312153217792 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpolrqpm8p\n",
            "Downloading: 100% 433/433 [00:00<00:00, 370kB/s]\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Lock 140312153217792 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Lock 140310754958304 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpqmiiriqx\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 85.7MB/s]\n",
            "storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "Lock 140310754958304 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "Lock 140310589095608 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpt1f5eqic\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 878kB/s]\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Lock 140310589095608 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Reading file /content/drive/My Drive/STAT946/DPR/nq-train-subset.json\n",
            "Aggregated data size: 1837\n",
            "Total cleaned data size: 1837\n",
            "  Total iterations per epoch=919\n",
            " Total updates=2757\n",
            "  Eval step = 919\n",
            "***** Training *****\n",
            "***** Epoch 0 *****\n",
            "Epoch: 0: Step: 1/919, loss=47.206924, lr=0.000000\n",
            "Train batch 100\n",
            "Avg. loss per last 100 batches: 6.828265\n",
            "Epoch: 0: Step: 101/919, loss=2.745196, lr=0.000010\n",
            "Train batch 200\n",
            "Avg. loss per last 100 batches: 2.202317\n",
            "Epoch: 0: Step: 201/919, loss=1.613019, lr=0.000010\n",
            "Train batch 300\n",
            "Avg. loss per last 100 batches: 1.168320\n",
            "Epoch: 0: Step: 301/919, loss=1.384292, lr=0.000009\n",
            "Train batch 400\n",
            "Avg. loss per last 100 batches: 0.890474\n",
            "Epoch: 0: Step: 401/919, loss=0.000042, lr=0.000009\n",
            "Train batch 500\n",
            "Avg. loss per last 100 batches: 0.832130\n",
            "Epoch: 0: Step: 501/919, loss=3.287738, lr=0.000008\n",
            "Train batch 600\n",
            "Avg. loss per last 100 batches: 0.883345\n",
            "Epoch: 0: Step: 601/919, loss=0.070874, lr=0.000008\n",
            "Train batch 700\n",
            "Avg. loss per last 100 batches: 1.089378\n",
            "Epoch: 0: Step: 701/919, loss=1.577706, lr=0.000008\n",
            "Train batch 800\n",
            "Avg. loss per last 100 batches: 1.202844\n",
            "Epoch: 0: Step: 801/919, loss=3.177922, lr=0.000007\n",
            "Train batch 900\n",
            "Avg. loss per last 100 batches: 0.803731\n",
            "Epoch: 0: Step: 901/919, loss=0.000000, lr=0.000007\n",
            "Validation: Epoch: 0 Step: 919/919\n",
            "NLL validation ...\n",
            "Reading file data/retriever/nq-dev.json\n",
            "Aggregated data size: 6515\n",
            "Total cleaned data size: 6515\n",
            "Eval step: 99 , used_time=47.059054 sec., loss=2.861231 \n",
            "Eval step: 199 , used_time=93.259349 sec., loss=0.001180 \n",
            "Eval step: 299 , used_time=139.907645 sec., loss=5.232149 \n",
            "Eval step: 399 , used_time=186.395812 sec., loss=1.936809 \n",
            "Eval step: 499 , used_time=232.975319 sec., loss=5.412921 \n",
            "Eval step: 599 , used_time=279.492818 sec., loss=0.410374 \n",
            "Eval step: 699 , used_time=325.974421 sec., loss=0.707308 \n",
            "Eval step: 799 , used_time=372.500578 sec., loss=6.119260 \n",
            "Eval step: 899 , used_time=419.019955 sec., loss=3.997693 \n",
            "Eval step: 999 , used_time=465.559853 sec., loss=4.080608 \n",
            "Eval step: 1099 , used_time=512.151219 sec., loss=0.176090 \n",
            "Eval step: 1199 , used_time=558.679299 sec., loss=1.514958 \n",
            "Eval step: 1299 , used_time=605.200047 sec., loss=1.808380 \n",
            "Eval step: 1399 , used_time=651.794091 sec., loss=3.424790 \n",
            "Eval step: 1499 , used_time=698.375569 sec., loss=0.121828 \n",
            "Eval step: 1599 , used_time=744.860783 sec., loss=1.679002 \n",
            "NLL Validation: loss = 1.718995. correct prediction ratio  4680/6516 ~  0.718232\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "Saved checkpoint at /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "Saved checkpoint to /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "New Best validation checkpoint /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "NLL validation ...\n",
            "Reading file data/retriever/nq-dev.json\n",
            "Aggregated data size: 6515\n",
            "Total cleaned data size: 6515\n",
            "Eval step: 99 , used_time=47.946748 sec., loss=2.861231 \n",
            "Eval step: 199 , used_time=93.724138 sec., loss=0.001180 \n",
            "Eval step: 299 , used_time=140.789873 sec., loss=5.232149 \n",
            "Eval step: 399 , used_time=187.279378 sec., loss=1.936809 \n",
            "Eval step: 499 , used_time=233.830235 sec., loss=5.412921 \n",
            "Eval step: 599 , used_time=280.601195 sec., loss=0.410374 \n",
            "Eval step: 699 , used_time=327.465606 sec., loss=0.707308 \n",
            "Eval step: 799 , used_time=374.127633 sec., loss=6.119260 \n",
            "Eval step: 899 , used_time=420.660378 sec., loss=3.997693 \n",
            "Eval step: 999 , used_time=467.186654 sec., loss=4.080608 \n",
            "Eval step: 1099 , used_time=513.691598 sec., loss=0.176090 \n",
            "Eval step: 1199 , used_time=560.349157 sec., loss=1.514958 \n",
            "Eval step: 1299 , used_time=606.858692 sec., loss=1.808380 \n",
            "Eval step: 1399 , used_time=653.557024 sec., loss=3.424790 \n",
            "Eval step: 1499 , used_time=700.059937 sec., loss=0.121828 \n",
            "Eval step: 1599 , used_time=746.725434 sec., loss=1.679002 \n",
            "NLL Validation: loss = 1.718995. correct prediction ratio  4680/6516 ~  0.718232\n",
            "Saved checkpoint at /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "Saved checkpoint to /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "Av Loss per epoch=1.737668\n",
            "epoch total correct predictions=1353\n",
            "***** Epoch 1 *****\n",
            "Epoch: 1: Step: 1/919, loss=0.001256, lr=0.000007\n",
            "Train batch 100\n",
            "Avg. loss per last 100 batches: 0.664267\n",
            "Epoch: 1: Step: 101/919, loss=0.001552, lr=0.000007\n",
            "Train batch 200\n",
            "Avg. loss per last 100 batches: 0.559230\n",
            "Epoch: 1: Step: 201/919, loss=0.146759, lr=0.000006\n",
            "Train batch 300\n",
            "Avg. loss per last 100 batches: 0.622491\n",
            "Epoch: 1: Step: 301/919, loss=0.000000, lr=0.000006\n",
            "Train batch 400\n",
            "Avg. loss per last 100 batches: 0.601515\n",
            "Epoch: 1: Step: 401/919, loss=0.006746, lr=0.000005\n",
            "Train batch 500\n",
            "Avg. loss per last 100 batches: 0.553952\n",
            "Epoch: 1: Step: 501/919, loss=1.399809, lr=0.000005\n",
            "Train batch 600\n",
            "Avg. loss per last 100 batches: 0.549278\n",
            "Epoch: 1: Step: 601/919, loss=0.000014, lr=0.000005\n",
            "Train batch 700\n",
            "Avg. loss per last 100 batches: 0.573149\n",
            "Epoch: 1: Step: 701/919, loss=0.000052, lr=0.000004\n",
            "Train batch 800\n",
            "Avg. loss per last 100 batches: 0.700850\n",
            "Epoch: 1: Step: 801/919, loss=0.000090, lr=0.000004\n",
            "Train batch 900\n",
            "Avg. loss per last 100 batches: 0.728455\n",
            "Epoch: 1: Step: 901/919, loss=0.000000, lr=0.000004\n",
            "Validation: Epoch: 1 Step: 919/919\n",
            "NLL validation ...\n",
            "Reading file data/retriever/nq-dev.json\n",
            "Aggregated data size: 6515\n",
            "Total cleaned data size: 6515\n",
            "Eval step: 99 , used_time=47.775366 sec., loss=0.368759 \n",
            "Eval step: 199 , used_time=93.518180 sec., loss=0.000084 \n",
            "Eval step: 299 , used_time=140.626701 sec., loss=3.753961 \n",
            "Traceback (most recent call last):\n",
            "  File \"train_dense_encoder.py\", line 564, in <module>\n",
            "    main()\n",
            "  File \"train_dense_encoder.py\", line 554, in main\n",
            "    trainer.run_train()\n",
            "  File \"train_dense_encoder.py\", line 129, in run_train\n",
            "    self._train_epoch(scheduler, epoch, eval_step, train_iterator)\n",
            "  File \"train_dense_encoder.py\", line 359, in _train_epoch\n",
            "    self.validate_and_save(epoch, train_data_iterator.get_iteration(), scheduler)\n",
            "  File \"train_dense_encoder.py\", line 145, in validate_and_save\n",
            "    validation_loss = self.validate_nll()\n",
            "  File \"train_dense_encoder.py\", line 174, in validate_nll\n",
            "    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_input, self.tensorizer, args)\n",
            "  File \"train_dense_encoder.py\", line 483, in _do_biencoder_fwd_pass\n",
            "    input.hard_negatives)\n",
            "  File \"train_dense_encoder.py\", line 458, in _calc_loss\n",
            "    hard_negatives_per_question)\n",
            "  File \"/content/DPR/dpr/models/biencoder.py\", line 183, in calc\n",
            "    loss = F.nll_loss(softmax_scores, torch.tensor(positive_idx_per_question).to(softmax_scores.device),\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-14480e81812d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cd DPR\\nls -sh \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\\nls -sh \"data/retriever/nq-dev.json\" \\n[ -d \"/content/drive/My Drive/STAT946/DPR/output/\" ] && echo \"Directory /content/drive/My Drive/STAT946/DPR/output/ exists.\"\\n\\npython train_dense_encoder.py \\\\\\n  --encoder_model_type hf_bert \\\\\\n  --pretrained_model_cfg bert-base-uncased \\\\\\n  --train_file \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\\\\\\n  --dev_file \"data/retriever/nq-dev.json\" \\\\\\n  --output_dir \"/content/drive/My Drive/STAT946/DPR/output\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 138\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cd DPR\nls -sh \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\nls -sh \"data/retriever/nq-dev.json\" \n[ -d \"/content/drive/My Drive/STAT946/DPR/output/\" ] && echo \"Directory /content/drive/My Drive/STAT946/DPR/output/ exists.\"\n\npython train_dense_encoder.py \\\n  --encoder_model_type hf_bert \\\n  --pretrained_model_cfg bert-base-uncased \\\n  --train_file \"/content/drive/My Drive/STAT946/DPR/nq-train-subset.json\"\\\n  --dev_file \"data/retriever/nq-dev.json\" \\\n  --output_dir \"/content/drive/My Drive/STAT946/DPR/output\"\n' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHiDGue_gFCp",
        "outputId": "cac6f3bb-b37c-4109-e012-7577394a0664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd DPR\n",
        "ls -sh \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\"\n",
        "ls -sh \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\"\n",
        "python generate_dense_embeddings.py \\\n",
        "  --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n",
        "  --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\n",
        "  --shard_id 0 \\ \n",
        "  --num_shards 1 \\\n",
        "  --out_file inference"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 3: cd: data/retriever: No such file or directory\n",
            "--2020-11-09 18:16:19--  https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 256239282 (244M) [application/gzip]\n",
            "Saving to: ‘nq-dev.json.gz’\n",
            "\n",
            "nq-dev.json.gz      100%[===================>] 244.37M  12.0MB/s    in 22s     \n",
            "\n",
            "2020-11-09 18:16:41 (11.3 MB/s) - ‘nq-dev.json.gz’ saved [256239282/256239282]\n",
            "\n",
            "778M nq-dev.json\n",
            "1.3M '/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv'\n",
            "ls: cannot access '/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919': No such file or directory\n",
            "usage: generate_dense_embeddings.py [-h]\n",
            "                                    [--pretrained_model_cfg PRETRAINED_MODEL_CFG]\n",
            "                                    [--encoder_model_type ENCODER_MODEL_TYPE]\n",
            "                                    [--pretrained_file PRETRAINED_FILE]\n",
            "                                    [--model_file MODEL_FILE]\n",
            "                                    [--projection_dim PROJECTION_DIM]\n",
            "                                    [--sequence_length SEQUENCE_LENGTH]\n",
            "                                    [--do_lower_case] [--no_cuda]\n",
            "                                    [--local_rank LOCAL_RANK] [--fp16]\n",
            "                                    [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                                    [--ctx_file CTX_FILE] --out_file OUT_FILE\n",
            "                                    [--shard_id SHARD_ID]\n",
            "                                    [--num_shards NUM_SHARDS]\n",
            "                                    [--batch_size BATCH_SIZE]\n",
            "generate_dense_embeddings.py: error: the following arguments are required: --out_file\n",
            "/bin/bash: line 15: --num_shards: command not found\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f15428c31109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\n# mkdir data/retriever_1\\ncd data/retriever\\nwget https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz -O nq-dev.json.gz\\ngzip -d nq-dev.json.gz\\nls -sh nq-dev.json\\n\\ncd DPR\\nls -sh \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\"\\nls -sh \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\"\\npython generate_dense_embeddings.py \\\\\\n  --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\\\\n  --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\\\\n  --shard_id 0 \\\\ \\n  --num_shards 1 \\\\\\n  --out_file inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 138\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '\n\n# mkdir data/retriever_1\ncd data/retriever\nwget https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz -O nq-dev.json.gz\ngzip -d nq-dev.json.gz\nls -sh nq-dev.json\n\ncd DPR\nls -sh \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\"\nls -sh \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\"\npython generate_dense_embeddings.py \\\n  --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n  --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\n  --shard_id 0 \\ \n  --num_shards 1 \\\n  --out_file inference' returned non-zero exit status 127."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCLOUe-zmtUH",
        "outputId": "4b1419aa-c77e-449a-f8ac-51c40fb9bad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd DPR\n",
        "\n",
        "python dense_retriever.py \\\n",
        "  --model_file \"/content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\" \\\n",
        "  --ctx_file \"/content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\" \\\n",
        "  --qa_file data/retriever/qas/nq-test.csv \\\n",
        "  --encoded_ctx_file inference_0.pkl \\\n",
        "  --out_file result \\\n",
        "  --n-docs 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized host ab29005f0866 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
            "16-bits training: False \n",
            " **************** CONFIGURATION **************** \n",
            "batch_size                     -->   32\n",
            "ctx_file                       -->   /content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\n",
            "device                         -->   cuda\n",
            "distributed_world_size         -->   1\n",
            "do_lower_case                  -->   False\n",
            "encoded_ctx_file               -->   inference_0.pkl\n",
            "encoder_model_type             -->   None\n",
            "fp16                           -->   False\n",
            "fp16_opt_level                 -->   O1\n",
            "hnsw_index                     -->   False\n",
            "index_buffer                   -->   50000\n",
            "local_rank                     -->   -1\n",
            "match                          -->   string\n",
            "model_file                     -->   /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "n_docs                         -->   10\n",
            "n_gpu                          -->   1\n",
            "no_cuda                        -->   False\n",
            "out_file                       -->   result\n",
            "pretrained_file                -->   None\n",
            "pretrained_model_cfg           -->   None\n",
            "projection_dim                 -->   0\n",
            "qa_file                        -->   data/retriever/qas/nq-test.csv\n",
            "save_or_load_index             -->   False\n",
            "sequence_length                -->   512\n",
            "validation_workers             -->   16\n",
            " **************** CONFIGURATION **************** \n",
            "Reading saved model from /content/drive/My Drive/STAT946/DPR/output/dpr_biencoder.0.919\n",
            "model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
            "Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-uncased\n",
            "Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert\n",
            "Overriding args parameter value from checkpoint state. Param = sequence_length, value = 512\n",
            "PyTorch version 1.7.0+cu101 available.\n",
            "2020-11-04 19:22:35.426842: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "TensorFlow version 2.3.0 available.\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "All model checkpoint weights were used when initializing HFBertEncoder.\n",
            "\n",
            "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Loading saved model state ...\n",
            "Encoder vector_size=768\n",
            "Reading all passages data from files: ['inference_0.pkl']\n",
            "Reading file inference_0.pkl\n",
            "Total data indexed 1999\n",
            "Data indexing completed.\n",
            "Encoded queries 800\n",
            "Encoded queries 1600\n",
            "Encoded queries 2400\n",
            "Encoded queries 3200\n",
            "Total encoded queries tensor torch.Size([3610, 768])\n",
            "index search time: 0.189437 sec.\n",
            "Reading data from: /content/drive/My Drive/STAT946/DPR/psgs_w100_subset.tsv\n",
            "Matching answers in top docs...\n",
            "Per question validation results len=3610\n",
            "Validation results: top k documents hits [54, 81, 109, 137, 150, 169, 183, 199, 212, 227]\n",
            "Validation results: top k documents hits accuracy [0.014958448753462604, 0.022437673130193906, 0.030193905817174516, 0.03795013850415512, 0.04155124653739612, 0.046814404432132965, 0.05069252077562327, 0.05512465373961219, 0.058725761772853186, 0.0628808864265928]\n",
            "Saved results * scores  to result\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP3FFQbF1ocT",
        "outputId": "ea807d6f-d7f6-421d-c6dd-51954fa42ddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "zip -r DPR.zip \"/content/DPR\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/DPR/ (stored 0%)\n",
            "  adding: content/DPR/CHANGELOG.md (stored 0%)\n",
            "  adding: content/DPR/train_reader.py (deflated 75%)\n",
            "  adding: content/DPR/data/ (stored 0%)\n",
            "  adding: content/DPR/data/download_data.py (deflated 84%)\n",
            "  adding: content/DPR/data/retriever/ (stored 0%)\n",
            "  adding: content/DPR/data/retriever/qas/ (stored 0%)\n",
            "  adding: content/DPR/data/retriever/qas/nq-dev.csv (deflated 61%)\n",
            "  adding: content/DPR/data/retriever/qas/README (deflated 37%)\n",
            "  adding: content/DPR/data/retriever/qas/nq-train.csv (deflated 61%)\n",
            "  adding: content/DPR/data/retriever/qas/nq-test.csv (deflated 60%)\n",
            "  adding: content/DPR/data/retriever/qas/LICENSE (deflated 67%)\n",
            "  adding: content/DPR/data/retriever/nq-dev.json (deflated 69%)\n",
            "  adding: content/DPR/.git/ (stored 0%)\n",
            "  adding: content/DPR/.git/info/ (stored 0%)\n",
            "  adding: content/DPR/.git/info/exclude (deflated 28%)\n",
            "  adding: content/DPR/.git/branches/ (stored 0%)\n",
            "  adding: content/DPR/.git/packed-refs (deflated 37%)\n",
            "  adding: content/DPR/.git/config (deflated 31%)\n",
            "  adding: content/DPR/.git/logs/ (stored 0%)\n",
            "  adding: content/DPR/.git/logs/HEAD (deflated 27%)\n",
            "  adding: content/DPR/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/DPR/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/DPR/.git/logs/refs/heads/master (deflated 27%)\n",
            "  adding: content/DPR/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/DPR/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/DPR/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n",
            "  adding: content/DPR/.git/objects/ (stored 0%)\n",
            "  adding: content/DPR/.git/objects/info/ (stored 0%)\n",
            "  adding: content/DPR/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/DPR/.git/objects/pack/pack-165113d88fe5c00893f1af303e60119d9db4aca1.idx (deflated 6%)\n",
            "  adding: content/DPR/.git/objects/pack/pack-165113d88fe5c00893f1af303e60119d9db4aca1.pack (deflated 1%)\n",
            "  adding: content/DPR/.git/description (deflated 14%)\n",
            "  adding: content/DPR/.git/hooks/ (stored 0%)\n",
            "  adding: content/DPR/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/DPR/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/DPR/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/DPR/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/DPR/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/DPR/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/DPR/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "  adding: content/DPR/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/DPR/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/DPR/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: content/DPR/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "  adding: content/DPR/.git/index (deflated 49%)\n",
            "  adding: content/DPR/.git/HEAD (stored 0%)\n",
            "  adding: content/DPR/.git/refs/ (stored 0%)\n",
            "  adding: content/DPR/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/DPR/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/DPR/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/DPR/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/DPR/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/DPR/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/DPR/dpr/ (stored 0%)\n",
            "  adding: content/DPR/dpr/models/ (stored 0%)\n",
            "  adding: content/DPR/dpr/models/fairseq_models.py (deflated 59%)\n",
            "  adding: content/DPR/dpr/models/hf_models.py (deflated 75%)\n",
            "  adding: content/DPR/dpr/models/__init__.py (deflated 77%)\n",
            "  adding: content/DPR/dpr/models/biencoder.py (deflated 72%)\n",
            "  adding: content/DPR/dpr/models/__pycache__/ (stored 0%)\n",
            "  adding: content/DPR/dpr/models/__pycache__/reader.cpython-36.pyc (deflated 52%)\n",
            "  adding: content/DPR/dpr/models/__pycache__/biencoder.cpython-36.pyc (deflated 51%)\n",
            "  adding: content/DPR/dpr/models/__pycache__/__init__.cpython-36.pyc (deflated 64%)\n",
            "  adding: content/DPR/dpr/models/__pycache__/hf_models.cpython-36.pyc (deflated 53%)\n",
            "  adding: content/DPR/dpr/models/reader.py (deflated 74%)\n",
            "  adding: content/DPR/dpr/models/pytext_models.py (deflated 69%)\n",
            "  adding: content/DPR/dpr/data/ (stored 0%)\n",
            "  adding: content/DPR/dpr/data/__init__.py (stored 0%)\n",
            "  adding: content/DPR/dpr/data/reader_data.py (deflated 74%)\n",
            "  adding: content/DPR/dpr/data/qa_validation.py (deflated 63%)\n",
            "  adding: content/DPR/dpr/data/__pycache__/ (stored 0%)\n",
            "  adding: content/DPR/dpr/data/__pycache__/qa_validation.cpython-36.pyc (deflated 47%)\n",
            "  adding: content/DPR/dpr/data/__pycache__/__init__.cpython-36.pyc (deflated 23%)\n",
            "  adding: content/DPR/dpr/data/__pycache__/reader_data.cpython-36.pyc (deflated 54%)\n",
            "  adding: content/DPR/dpr/__init__.py (stored 0%)\n",
            "  adding: content/DPR/dpr/utils/ (stored 0%)\n",
            "  adding: content/DPR/dpr/utils/tokenizers.py (deflated 68%)\n",
            "  adding: content/DPR/dpr/utils/__init__.py (stored 0%)\n",
            "  adding: content/DPR/dpr/utils/dist_utils.py (deflated 61%)\n",
            "  adding: content/DPR/dpr/utils/data_utils.py (deflated 66%)\n",
            "  adding: content/DPR/dpr/utils/__pycache__/ (stored 0%)\n",
            "  adding: content/DPR/dpr/utils/__pycache__/tokenizers.cpython-36.pyc (deflated 58%)\n",
            "  adding: content/DPR/dpr/utils/__pycache__/__init__.cpython-36.pyc (deflated 23%)\n",
            "  adding: content/DPR/dpr/utils/__pycache__/model_utils.cpython-36.pyc (deflated 51%)\n",
            "  adding: content/DPR/dpr/utils/__pycache__/data_utils.cpython-36.pyc (deflated 50%)\n",
            "  adding: content/DPR/dpr/utils/__pycache__/dist_utils.cpython-36.pyc (deflated 42%)\n",
            "  adding: content/DPR/dpr/utils/model_utils.py (deflated 68%)\n",
            "  adding: content/DPR/dpr/options.py (deflated 68%)\n",
            "  adding: content/DPR/dpr/indexer/ (stored 0%)\n",
            "  adding: content/DPR/dpr/indexer/faiss_indexers.py (deflated 71%)\n",
            "  adding: content/DPR/dpr/indexer/__pycache__/ (stored 0%)\n",
            "  adding: content/DPR/dpr/indexer/__pycache__/faiss_indexers.cpython-36.pyc (deflated 56%)\n",
            "  adding: content/DPR/dpr/__pycache__/ (stored 0%)\n",
            "  adding: content/DPR/dpr/__pycache__/options.cpython-36.pyc (deflated 49%)\n",
            "  adding: content/DPR/dpr/__pycache__/__init__.cpython-36.pyc (deflated 24%)\n",
            "  adding: content/DPR/README.md (deflated 64%)\n",
            "  adding: content/DPR/generate_dense_embeddings.py (deflated 61%)\n",
            "  adding: content/DPR/CONTRIBUTING.md (deflated 45%)\n",
            "  adding: content/DPR/preprocess_reader_data.py (deflated 56%)\n",
            "  adding: content/DPR/train_dense_encoder.py (deflated 75%)\n",
            "  adding: content/DPR/inference_0.pkl (deflated 8%)\n",
            "  adding: content/DPR/setup.py (deflated 46%)\n",
            "  adding: content/DPR/CODE_OF_CONDUCT.md (deflated 55%)\n",
            "  adding: content/DPR/dense_retriever.py (deflated 69%)\n",
            "  adding: content/DPR/result (deflated 72%)\n",
            "  adding: content/DPR/LICENSE (deflated 68%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkC4rBjy9G3A",
        "outputId": "7d460b0f-1f6a-4dfd-8713-0ed28a00d872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "%%shell\n",
        "cd DPR\n",
        "python data/download_data.py --resource data.retriever.trivia-train\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: DPR: No such file or directory\n",
            "python3: can't open file 'data/download_data.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46084310c984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cd DPR\\npython data/download_data.py --resource data.retriever.trivia-train\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 138\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'cd DPR\npython data/download_data.py --resource data.retriever.trivia-train\n' returned non-zero exit status 2."
          ]
        }
      ]
    }
  ]
}